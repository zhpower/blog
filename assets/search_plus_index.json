{"/blog/jekyll/2022-06-22-misc.html": {
    "title": "misc",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2022-06-22-misc.html",
    "body": "kref kref_init 是一个用于初始化内核对象引用计数器（krefs）的函数。它允许你为你的对象添加引用计数，确保在多个地方使用和传递对象时，代码的正确性。以下是关于 kref_init 的一些重要信息： 初始化： 在分配内存并创建对象后，你需要调用 kref_init 来初始化引用计数器。例如： struct my_data *data; data = kmalloc(sizeof(*data), GFP_KERNEL); if (!data) return -ENOMEM; kref_init(&amp;data-&gt;refcount); 这将在 kref 中的 refcount 设置为 1。 使用规则： 在对指针进行非临时拷贝（尤其是传递给另一个执行线程）之前，必须使用 kref_get 增加引用计数。 在完成对指针的处理后，必须调用 kref_put。如果这是对指针的最后一次引用，释放程序将被调用。 示例： void data_release(struct kref *ref) { struct my_data *data = container_of(ref, struct my_data, refcount); kfree(data); } void more_data_handling(void *cb_data) { struct my_data *data = cb_data; // 处理 data kref_put(&amp;data-&gt;refcount, data_release); } int my_data_handler(void) { int rv = 0; struct my_data *data; data = kmalloc(sizeof(*data), GFP_KERNEL); if (!data) return -ENOMEM; kref_init(&amp;data-&gt;refcount); kref_get(&amp;data-&gt;refcount); // 创建线程处理数据 // ... kref_put(&amp;data-&gt;refcount, data_release); return rv; } 在上述示例中，两个线程处理数据的顺序并不重要，kref_put 会在数据不再被引用时释放它。 请注意，遵循这些规则可以确保正确管理内核对象的引用计数，避免内存泄漏和悬挂指针。 IDR(ID Range) IDR 是一种用于管理连续整数范围的数据结构，通常用于内核中需要为对象分配唯一标识符的场景。 idr_alloc 函数用于在 Linux 内核中分配 IDR（ID Range）对象中的未使用的 ID。 以下是 idr_alloc 函数的用法： 首先，您需要初始化一个 IDR。对于静态分配的 IDR，您可以使用 DEFINE_IDR() 宏；对于动态分配的 IDR，您可以使用 idr_init() 函数。 调用 idr_alloc() 来分配一个未使用的 ID。 使用 idr_find() 查询与该 ID 相关的指针。 使用 idr_remove() 释放该 ID。 如果需要更改与某个 ID 相关联的指针，您可以调用 idr_replace()。这通常用于保留 ID，通过将 NULL 指针传递给分配函数，然后使用保留的 ID 初始化对象，最后将初始化的对象插入 IDR。 到目前为止，所有用户都满足了 UINT_MAX 的限制，因此他们使用 idr_alloc_u32()。 如果需要按顺序分配 ID，您可以使用 idr_alloc_cyclic()。请注意，处理较大数量的 ID 时，IDR 的效率会降低，因此使用这个函数会有一些代价。 当您使用完 IDR 后，可以调用 idr_destroy() 来释放 IDR 占用的内存。这不会释放 IDR 指向的对象；如果您想这样做，请使用其中一个迭代器来执行此操作。 您可以使用 idr_is_empty() 来查看当前是否分配了任何 ID。 如果在从 IDR 分配一个新 ID 时需要带锁，您可能需要传递一组限制性的 GFP 标志，但这可能导致 IDR 无法分配内存。为了解决该问题，您可以在获取锁之前调用 idr_preload()，然后在分配之后调用 idr_preload_end()。 #include &lt;linux/idr.h&gt; int main(void) { struct idr my_idr; int id1, id2; void *ptr1, *ptr2; // Initialize the IDR idr_init(&amp;my_idr); // Allocate two unused IDs id1 = idr_alloc(&amp;my_idr, \"sample1\", 0, 0, GFP_KERNEL); id2 = idr_alloc(&amp;my_idr, \"sample2\", 0, 0, GFP_KERNEL); // Associate pointers with the IDs ptr1 = (void *)0xdeadbeef; ptr2 = (void *)0xcafebabe; idr_replace(&amp;my_idr, ptr1, id1); idr_replace(&amp;my_idr, ptr2, id2); // Look up pointers by ID ptr1 = idr_find(&amp;my_idr, id1); ptr2 = idr_find(&amp;my_idr, id2); // Free the IDs idr_remove(&amp;my_idr, id1); idr_remove(&amp;my_idr, id2); // Destroy the IDR idr_destroy(&amp;my_idr); return 0; } 非一致性内存 和一致性 dma_alloc_noncoherent 它是Linux内核中的一个DMA内存分配函数，用于分配一段物理内存，使其可以被DMA硬件访问12. 这个函数的作用是在非一致性内存（non-coherent memory）上分配一块区域，以便设备可以使用它作为DMA的源或目标地址。让我详细解释一下这个函数的用途和参数。 dma_alloc_noncoherent函数的原型如下： void *dma_alloc_noncoherent(struct device *dev, size_t size, dma_addr_t *dma_handle, gfp_t flag); dev: 指向设备结构的指针，表示要为哪个设备分配内存。 size: 要分配的内存大小（以字节为单位）。 dma_handle: 用于返回DMA地址的指针。这个地址可以转换为与总线宽度相同的无符号整数，并传递给设备作为分配区域的DMA地址基址。 flag: 用于指定内存分配的GFP_标志（类似于kmalloc()中的标志）。例如，可以使用GFP_KERNEL来分配普通内核内存。 非一致性内存是一种特殊类型的内存，写入它的数据可以立即被处理器或设备读取，而无需考虑缓存效应。需要注意的是，CPU不能直接引用dma_addr_t，因为物理地址空间和DMA地址空间之间可能存在转换。 使用dma_alloc_noncoherent分配的内存区域不保证一致性，因此在使用之前，可能需要手动刷新处理器的写缓冲区，以确保设备可以正确读取该内存。 释放由dma_alloc_noncoherent分配的内存时，应使用dma_free_noncoherent函数： void dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr, dma_addr_t dma_handle); dev、size和dma_handle参数必须与传递给dma_alloc_noncoherent的相同。 cpu_addr是由dma_alloc_noncoherent返回的虚拟地址。 请注意，与其他内存分配函数不同，这些函数只能在启用IRQ的情况下调用。 如果你的驱动程序需要大量较小的DMA一致性内存区域，你可以使用DMA池（dma_pool）来分配和管理这些区域，而不是使用dma_alloc_coherent()。DMA池类似于kmem_cache，但它使用dma_alloc_coherent()而不是__get_free_pages() dma_alloc_wc 这个函数允许驱动程序申请带缓存一致性的DMA内存。缓存一致性是指确保CPU和DMA设备之间的数据一致性，以避免数据不一致的问题。使用dma_alloc_wc分配的内存区域旨在在CPU和DMA设备之间保持一致，以便数据正确传输。 释放由dma_alloc_wc分配的内存时，应使用dma_free_wc函数： void dma_free_wc(struct device *dev, size_t size, void *cpu_addr, dma_addr_t dma_handle); dev、size和dma_handle参数必须与传递给dma_alloc_wc的相同。 cpu_addr是由dma_alloc_wc返回的虚拟地址。 adf timer_setup 它是Linux内核中用于初始化定时器的函数。它能够方便地设置和初始化一个计时器，并通过设置参数来灵活地控制计时器的行为1. 合理使用timer_setup函数可以让我们更好地处理时间相关的任务，提高操作系统的性能和可靠性。 在Linux内核中，定时器通常使用timer_list结构体来表示。下面是timer_list结构体的一些关键字段： entry: 定时器列表元素，用于将定时器挂载在内核定时器链表上。 expires: 定时器定时时间。 function: 定时器回调函数，定时器时间到时执行该函数。 flags: 标志位，用于设置定时器的属性。 在旧版本的内核中，我们使用init_timer函数来初始化定时器。而在新版本中，这个函数变成了timer_setup函数。下面是timer_setup函数的定义： void timer_setup(struct timer_list *timer, void (*callback)(struct timer_list *), unsigned int flags); 使用timer_setup函数时，我们需要传入以下参数： timer: 要初始化的定时器。 callback: 定时器的回调函数，此函数的形参是当前定时器的变量。 flags: 标志位，可以设置定时器的属性。 #include &lt;linux/kernel.h&gt; #include &lt;linux/module.h&gt; #include &lt;linux/timer.h&gt; MODULE_LICENSE(\"GPL\"); static struct timer_list my_timer; void my_timer_callback(struct timer_list *timer) { printk(KERN_ALERT \"This line is printed after 5 seconds.\\n\"); } static int init_module_with_timer(void) { printk(KERN_ALERT \"Initializing a module with timer.\\n\"); // Setup the timer for initial use timer_setup(&amp;my_timer, my_timer_callback, 0); // Set the timer interval to 5000 milliseconds (5 seconds) mod_timer(&amp;my_timer, jiffies + msecs_to_jiffies(5000)); return 0; } static void exit_module_with_timer(void) { printk(KERN_ALERT \"Goodbye, cruel world!\\n\"); del_timer(&amp;my_timer); } module_init(init_module_with_timer); module_exit(exit_module_with_timer); kthread_create_worker 函数是Linux内核中用于创建内核线程的一个函数。通过设置标志参数和格式化字符串，可以指定创建内核线程的行为和名称。它分配并初始化了一个kthread_worker结构体，并使用它来创建内核线程. 以下是kthread_create_worker函数的一些关键参数： cpu: 如果大于等于0，将创建特定于某个CPU的工作线程；如果不想创建特定于CPU的工作线程，可以将CPU域赋值为-1。 flags: 可以设置一些标志位，根据需要来控制内核线程的行为。 namefmt: 一个格式化字符串，用于指定内核线程的名称。 这个函数会分配内存并初始化kthread_worker结构，然后返回指向该结构的指针。您可以根据具体需求使用这个函数来创建和管理内核线程。 如果您需要一个示例代码，以下是一个简单的例子，展示了如何在模块初始化时创建一个内核线程，以及如何在卸载模块时关闭该内核线程： #include &lt;linux/module.h&gt; #include &lt;linux/kthread.h&gt; #include &lt;linux/delay.h&gt; MODULE_LICENSE(\"GPL\"); static int demo_thr(void *data) { while (!kthread_should_stop()) { msleep_interruptible(2000); printk(KERN_INFO \"Thread is running...\\n\"); } return 0; } static struct task_struct *thr = NULL; static int kthread_demo_init(void) { thr = kthread_run(demo_thr, NULL, \"kthread-demo\"); if (!thr) { printk(KERN_ERR \"Failed to create kthread\\n\"); return -ENOMEM; } return 0; } static void kthread_demo_exit(void) { if (thr) { kthread_stop(thr); thr = NULL; } } module_init(kthread_demo_init); module_exit(kthread_demo_exit); 在这个示例中，我们使用kthread_run函数创建一个名为kthread-demo的内核线程，它每隔2秒打印一条信息。在卸载模块时，我们使用kthread_stop来关闭该内核线程。 schedule_work 函数是Linux内核中的一个重要函数，用于将一个工作项（work）添加到工作队列（workqueue）中。这个函数的作用是在后台执行一些延迟较长的任务，而不会阻塞主线程的执行。 以下是关于schedule_work函数的一些要点： 功能：将工作项添加到默认的工作队列（通常是system_wq）中，以便稍后执行。 调用方式：schedule_work(&amp;my_work);，其中my_work是一个已经初始化的工作项。 工作队列：工作队列是一种异步执行机制，用于处理延迟的或非实时的任务。 延迟执行：schedule_work会将工作项添加到工作队列中，等待系统调度执行。这样，主线程可以继续执行其他任务，而不必等待工作项完成。 工作项回调函数：工作项的实际执行逻辑由回调函数定义。当工作项被调度执行时，会调用这个回调函数。 以下是一个简单的示例代码，展示了如何使用INIT_WORK和schedule_work来创建和调度一个工作项： #include &lt;linux/init.h&gt; #include &lt;linux/module.h&gt; #include &lt;linux/kernel.h&gt; #include &lt;linux/workqueue.h&gt; MODULE_LICENSE(\"GPL\"); static struct workqueue_struct *my_workqueue; static struct work_struct my_work; // 工作项的回调函数 static void my_work_handler(struct work_struct *work) { printk(KERN_INFO \"My work handler is running...\\n\"); // 在这里执行您的工作逻辑 } static int init_my_module(void) { printk(KERN_INFO \"Initializing my kernel module with workqueue...\\n\"); // 创建工作队列 my_workqueue = create_singlethread_workqueue(\"my_workqueue\"); if (!my_workqueue) { printk(KERN_ERR \"Failed to create workqueue\\n\"); return -ENOMEM; } // 初始化工作项 INIT_WORK(&amp;my_work, my_work_handler); // 将工作项添加到工作队列 schedule_work(&amp;my_work); return 0; } static void cleanup_my_module(void) { printk(KERN_INFO \"Cleaning up my kernel module...\\n\"); // 销毁工作队列 destroy_workqueue(my_workqueue); } module_init(init_my_module); module_exit(cleanup_my_module); 在这个示例中，我们首先定义了一个名为my_workqueue的工作队列结构体，以及一个名为my_work的工作项。然后，在init_my_module函数中使用create_singlethread_workqueue来创建一个名为my_workqueue的工作队列。接着，我们使用INIT_WORK来初始化工作项，并使用schedule_work来调度它。 DRM Blob 当涉及到 DRM（Direct Rendering Manager）中的“blob”时，我们实际上在讨论一种特定类型的属性。让我详细解释一下： 什么是 Blob？ 在 DRM 中，Blob 是一种特殊的属性，用于存储自定义数据块。它允许用户空间应用程序将自定义结构体数据传递给内核空间。 Blob 通常用于存储一些不适合使用标准属性的数据，例如模式信息、LUT（查找表）数据、校准数据等。 Blob 的结构和用法： Blob 由两部分组成： Blob ID：每个 Blob 都有一个唯一的 ID，用于在内核中标识该 Blob。 Blob 数据：这是一个自定义长度的内存块，可以存储任何类型的数据。 Blob 可以存储各种信息，例如显示模式（mode）的详细信息、颜色校准数据、Gamma 表等。 Blob 的示例用途： 模式信息（Mode Information）：Blob 可以存储显示模式的详细信息，例如分辨率、刷新率、像素格式等。 颜色校准数据：如果您需要在显示设备上进行颜色校准，可以使用 Blob 存储校准数据。 Gamma 表：Gamma 表用于调整显示设备的亮度和对比度。这些数据可以存储在 Blob 中。 如何操作 Blob？ 用户空间应用程序可以通过 DRM 接口来创建、获取和设置 Blob。 创建 Blob：使用 drmModeCreatePropertyBlob 函数来创建一个 Blob，并将自定义数据传递给内核。 获取 Blob 数据：使用 drmModeGetPropertyBlob 函数来获取 Blob 中存储的数据。 设置 Blob 数据：使用 drmModeAtomicAddProperty 函数将 Blob ID 添加到 Atomic 请求中，从而修改 Blob 数据。 总之，Blob 是一种用于存储自定义数据的特殊属性，允许用户空间应用程序与内核交换非标准化的信息。它在 DRM 中的应用范围很广，例如显示模式、颜色校准和 Gamma 表等。 completion init_completion() 是Linux内核中用于完成事件通知机制的一个函数，主要用于进程间或线程间的同步。这个函数初始化一个 completion 结构体，该结构体用于表示某个事件是否已经发生。在多线程或多进程编程中，有时需要一个线程或进程等待另一个线程或进程完成某个任务。 让我们来详细了解一下 init_completion() 函数的功能和用法： 初始化completion结构体： completion 结构体用于维护“complete”状态，表示某个任务是否已完成。 结构体定义如下： struct completion { unsigned int done; struct swait_queue_head wait; }; done 字段表示完成状态，初始值为 0。 swait_queue_head 是一个等待队列头，用于管理等待该完成事件的线程。 init_completion() 函数： 动态定义及初始化一个信号量： #define init_completion(x) __init_completion(x) static inline void __init_completion(struct completion *x) { x-&gt;done = 0; init_swait_queue_head(&amp;x-&gt;wait); } 这个函数实际上是初始化了 completion 结构体中的信号量。 等待完成： 等待信号量的释放： void __sched wait_for_completion(struct completion *x) { wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE); } 发信端： complete() 函数用于唤醒等待该完成事件的单个线程： void complete(struct completion *x) { unsigned long flags; raw_spin_lock_irqsave(&amp;x-&gt;wait.lock, flags); if (x-&gt;done != UINT_MAX) x-&gt;done++; swake_up_locked(&amp;x-&gt;wait); raw_spin_unlock_irqrestore(&amp;x-&gt;wait.lock, flags); } 同时唤醒所有等待线程： complete_all() 函数用于唤醒等待此特定完成事件的所有线程： void complete_all(struct completion *x) { unsigned long flags; lockdep_assert_RT_in_threaded_ctx(); raw_spin_lock_irqsave(&amp;x-&gt;wait.lock, flags); x-&gt;done = UINT_MAX; swake_up_all_locked(&amp;x-&gt;wait); raw_spin_unlock_irqrestore(&amp;x-&gt;wait.lock, flags); } 完整示例 #include &lt;linux/module.h&gt; #include &lt;linux/init.h&gt; #include &lt;linux/completion.h&gt; #include &lt;linux/delay.h&gt; #include &lt;linux/kthread.h&gt; MODULE_LICENSE(\"GPL\"); MODULE_AUTHOR(\"kevin\"); static struct completion my_completion; static int my_thread(void *data){ pr_info(\"My thread is waiting for completion...\\n\"); wait_for_completion(&amp;my_completion); pr_info(\"My thread woke up! Event completed.\\n\"); return 0; } static int __init my_init(void){ pr_info(\"Initializing my module...\\n\"); init_completion(&amp;my_completion); // Start a new kernel thread kthread_run(my_thread, NULL, \"my_thread\"); // Simulate some work... msleep(2000); pr_info(\"Completing the event...\\n\"); complete(&amp;my_completion); return 0; } static void __exit my_exit(void){ pr_info(\"Exiting my module...\\n\"); } module_init(my_init); module_exit(my_exit); benchmark DRM kernel aspects (display and render): IGT GPU Tools (IGT): main DRM test suite, used for CI https://gitlab.freedesktop.org/drm/igt-gpu-tools/ OpenGL aspects: drawElements Quality Program (dEQP): OpenGL/OpenGL ES/Vulkan conformance tests https://android.googlesource.com/platform/external/deqp/ glmark2: OpenGL 2.0 and ES 2.0 benchmark tool https://github.com/glmark2/glmark2/ Patch series continuous integration: EzBench: a collection of tools to benchmark graphics-related patch-series https://github.com/freedesktop/ezbench/ General benchmarking (including graphics): Phoronix Test Suite: automated benchmarking tool https://github.com/phoronix-test-suite/phoronix-test-suite/"
  },"/blog/jekyll/2022-04-27-bit.html": {
    "title": "位运算",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2022-04-27-bit.html",
    "body": "1.位运算概述 从现代计算机中所有的数据二进制的形式存储在设备中。即 0、1 两种状态，计算机对二进制数据进行的运算(+、-、*、\\/)都是叫位运算，即将符号位共同参与运算的运算。 口说无凭，举一个简单的例子来看下 CPU 是如何进行计算的，比如这行代码： int a = 35; int b = 47; int c = a + b; 计算两个数的和，因为在计算机中都是以二进制来进行运算，所以上面我们所给的 int 变量会在机器内部先转换为二进制在进行相加： 35: 0 0 1 0 0 0 1 1 47: 0 0 1 0 1 1 1 1 ———————————————————— 82: 0 1 0 1 0 0 1 0 所以，相比在代码中直接使用(+、-、*、/ )运算符，合理的运用位运算更能显著提高代码在机器上的执行效率。 2.位运算概览 ![[image-20230313173955848.png]] 3.与运算符&amp; 定义：参加运算的两个数据，按二进制位进行”与”运算。 3.1 运算规则： 0&amp;0=0 0&amp;1=0 1&amp;0=0 1&amp;1=1 总结：两位同时为1，结果才为1，否则结果为0。 例如：3&amp;5 即 0000 0011&amp; 0000 0101 = 0000 0001，因此 3&amp;5 的值得1。 注意：负数按补码形式参加按位与运算。 3.2 与运算符用途 3.2.1清零 如果想将一个单元清零，即使其全部二进制位为0，只要与一个各位都为零的数值相与，结果为零。 3.2.2取一个数的指定位 比如取数 X=1010 1110 的低4位，只需要另找一个数Y，令Y的低4位为1，其余位为0，即Y=0000 1111，然后将X与Y进行按位与运算（X&amp;Y=0000 1110）即可得到X的指定位。 3.2.3判断奇偶 只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a \\&amp; 1) == 0)代替if (a \\% 2 == 0)来判断a是不是偶数。 4 或运算符| 定义：参加运算的两个对象，按二进制位进行”或”运算。 4.1运算规则： 0 0=0 0 1=1 1 0=1 1 1=1 总结：参加运算的两个对象只要有一个为1，其值为1。 例如：3 5即 0000 0011 0000 0101 = 0000 0111，因此，3 5的值得7。　 注意：负数按补码形式参加按位或运算。 4.2或运算的用途： 4.2.1常用来对一个数据的某些位设置为1 比如将数 X=1010 1110 的低4位设置为1，只需要另找一个数Y，令Y的低4位为1，其余位为0，即Y=0000 1111，然后将X与Y进行按位或运算（X Y=1010 1111）即可得到。 5异或运算符^ 定义：参加运算的两个数据，按二进制位进行”异或”运算。 5.1 运算规则： 0^0=0 0^1=1 1^0=1 1^1=0 总结：参加运算的两个对象，如果两个相应位相同为0，相异为1。 异或的几条性质: 1、交换律 2、结合律 (a^b)^c == a^(b^c) 3、对于任何数x，都有 x^x=0，x^0=x 4、自反性: a^b^b=a^0=a; 5.2 异或运算的用途： 5.2.1翻转指定位 比如将数 X=1010 1110 的低4位进行翻转，只需要另找一个数Y，令Y的低4位为1，其余位为0，即Y=0000 1111，然后将X与Y进行异或运算（X^Y=1010 0001）即可得到。 5.2.2与0相异或值不变 例如：1010 1110 ^ 0000 0000 = 1010 1110 5.2.3交换两个数 void Swap(int &amp;a, int &amp;b){     if (a != b){         a ^= b;         b ^= a;         a ^= b;     } } 6取反运算符~ 定义：参加运算的一个数据，按二进制进行”取反”运算。 6.1 运算规则：　 ~1=0 ~0=1 总结：对一个二进制数按位取反，即将0变1，1变0。 6.2 取反运算符的用途： 6.2.1使一个数的最低位为零 使a的最低位为0，可以表示为：a &amp; ~1。~1的值为 1111 1111 1111 1110，再按”与”运算，最低位一定为0。因为” ~”运算符的优先级比算术运算符、关系运算符、逻辑运算符和其他运算符都高。 7.左移运算符« 定义：将一个运算对象的各二进制位全部左移若干位（左边的二进制位丢弃，右边补0）。 设 a=1010 1110，a = a« 2 将a的二进制位左移2位、右补0，即得a=1011 1000。 7.1 «1 左移 1位 相当于乘2 若左移时舍弃的高位不包含1，则每左移一位，相当于该数乘以2。 8右移运算符» 定义：将一个数的各二进制位全部右移若干位，正数左补0，负数左补1，右边丢弃。 例如：a=a»2 将a的二进制位右移2位，左补0 或者 左补1得看被移数是正还是负。 8.1 »1 右移一位相当于除2 操作数每右移一位，相当于该数除以2。 9复合赋值运算符 位运算符与赋值运算符结合，组成新的复合赋值运算符，它们是： &amp;= 例：a&amp;=b 相当于 a=a&amp;b = 例：a =b 相当于 a=a b &gt;&gt;= 例：a»=b 相当于 a=a»b «= 例：a«=b 相当于 a=a« b ^= 例：a^=b 相当于 a=a^b 运算规则：和前面讲的复合赋值运算符的运算规则相似。 不同长度的数据进行位运算：如果两个不同长度的数据进行位运算时，系统会将二者按右端对齐，然后进行位运算。 以”与运算”为例说明如下：我们知道在C语言中long型占4个字节，int型占2个字节，如果一个long型数据与一个int型数据进行”与运算”，右端对齐后，左边不足的位依下面三种情况补足， 1）如果整型数据为正数，左边补16个0。 2）如果整型数据为负数，左边补16个1。 3）如果整形数据为无符号数，左边也补16个0。 如：long a=123；int b=1；计算a&amp; b。 如：long a=123；int b=-1；计算a&amp; b。 如：long a=123；unsigned intb=1；计算a &amp; b。"
  },"/blog/jekyll/2021-04-27-ffmpeg.html": {
    "title": "ffmpeg命令",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2021-04-27-ffmpeg.html",
    "body": "01. 视频分割 ffmpeg -ss 00:00:00 -i input.mp4 -c copy -t 60 output.mp4 -ss 表示视频分割的起始时间，-t 表示分割时长，同时也可以用 00:01:00表示 ==注意== ：-ss 要放在 -i 之前 02. 视频区域裁剪： ffmpeg -i 3.mkv -filter_complex crop=1024:50:0:550 -y 4.mkv 03. 视频预览 ffplay 3.mkv -vf crop=1024:50:0:550 04. 视频放大,缩小 ffmpeg -i 2.mp4 -vf \"scale=1280:64\" 4.mp4 ==scale== =w:h 表示放大后的大小 05. 列出所有format // list all pix format ffmpeg -pix_fmts 06. 图片转换 ffmpeg -i temp.jpg -s 1024x680 -pix_fmt yuvj420p 9.yuv ffmpeg.exe -i agf-dog-1280x960.jpg -vf scale=1920:1080 agf-dog-1920x1080.jpg 07. 视频format转换 YUV -&gt; RGB ffmpeg -s 360x270 -pix_fmt yuv420p -i  2_test_360x270_50.yuv -pix_fmt rgb24  aaaa.rgb ffmpeg -s 640x480 -pix_fmt nv12 -i 640x480_1.jpg -vf scale=640:480,setsar=1:1 640x480_1_nv12.yuv -hide_banner 08. 视频叠加 ffmpeg -i input1 -i input2 -filter_complex overlay=x:y output 09. 视频旋转 //mp4向左旋转90度 ffmpeg -i input.mp4 -metadata:s:v rotate=”90” -codec copy outut.mp4 //mp4向右旋转90度 ffmpeg -i input.mp4 -metadata:s:v rotate=”-90” -codec copy outut.mp4 10. 视频镜像 //mp4左右镜像翻转 ffmpeg -i input.mp4 -vf \"hflip\" outut.mp4 //mp4上下镜像翻转 ffmpeg -i input.mp4 -vf \"vflip\" outut.mp4 10. 图片旋转 //图片向右旋转90度 ffmpeg -i input.png -vf rotate='90*PI/180' -y rotate60.png //图片向右旋转90度 ffmpeg -i input.png -vf rotate='-90*PI/180' -y rotate_90.png //图片像左旋转90度 ffmpeg -i input.png -vf transpose=2 -y transpose2.png //图片像右旋转90度 ffmpeg -i input.png -vf transpose=1 -y transpose2.png //逆时针(向左)旋转90°，然后垂直（上下）翻转 ffmpeg -i input.png -vf transpose=0 -y transpose0.png 11. 图片镜像 //图片左右镜像翻转 ffmpeg -i input.png -vf hflip -y hflip.png //图片上下镜像翻转 ffmpeg -i input.png -vf vflip -y vflip.png //yuv数据左右镜像翻转 ffmpeg -s 1920x1080 -pix_fmt nv12 -i nv12_1.yuv -vf hflip -y hflip_nv12.yuv //播放左右翻转后的yuv数据 ffplay -video_size 1920x1080 -pixel_format nv12 hflip_nv12.yuv 12. 音频音量调节大小 //音量翻倍，写在滤镜里 ffmpeg -i input.wav -af volume=2 -y output.wav //音量翻倍，不写在滤镜中 ffmpeg -i input.wav -vol 2000 -y output.wav 13. 调节播放速度 ffmpeg -i test1.mp4 -vf \"setpts=0.25*PTS\" test2.mp4 四倍慢速： ffmpeg -i test1.mp4 -vf \"setpts=4*PTS\" test2.mp4"
  },"/blog/jekyll/2020-08-20-OpenGL_misc.html": {
    "title": "OpenGL Misc",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2020-08-20-OpenGL_misc.html",
    "body": "glInvalidateFramebuffer vs glClear glInvalidateFramebuffer 和 glClear 是OpenGL中两个不同的函数，它们的作用和用法有所不同： glInvalidateFramebuffer： 功能：glInvalidateFramebuffer 用于显式地标记帧缓冲区的某些部分为无效。这样，GPU就知道这些部分的内容不再需要，可以避免不必要的数据交换。 应用场景：通常在使用帧缓冲区对象（FBO）时，当我们切换到不同的FBO或者不再需要某些颜色、深度或模板缓冲区的内容时，可以调用该函数。 性能影响：尽管在RenderDoc等工具中可能显示glInvalidateFramebuffer的耗时较高，但实际上，只有少数几次调用不会对渲染性能产生影响。 glClear： 功能：glClear 用于清除当前帧缓冲区的内容，包括颜色缓冲区、深度缓冲区和模板缓冲区。 应用场景：在每一帧开始时，我们通常会调用glClear来准备帧缓冲区，以便进行新的绘制。 性能影响：glClear的性能开销通常较小，但在某些情况下，如果频繁调用，可能会影响性能。 总结： glInvalidateFramebuffer 用于标记帧缓冲区的部分内容为无效，以减少不必要的数据交换。 glClear 用于清除整个帧缓冲区的内容，以准备进行新的绘制。 在使用RenderDoc等工具时，可以忽略glInvalidateFramebuffer的耗时，但需要关注片上高速缓存回写内存的消耗 。 glDiscardFramebufferEXT void glDiscardFramebufferEXT(enum target, sizei numAttachments, const enum *attachments); 这个扩展提供了一个新的命令，glDiscardFramebufferEXT，它会使得指定帧缓冲附件的内容变为未定义状态。在未来的操作修改内容之前，这些指定缓冲区的内容是未定义的，只有被修改的区域保证包含有效内容。有效地使用此命令可以为实现提供新的优化机会。 一些 OpenGL ES 实现会将帧缓冲图像缓存到一个小的快速内存池中。在渲染之前，这些实现必须将逻辑缓冲区（如颜色、深度、模板等）的现有内容加载到该内存中。渲染后，这些缓冲区中的一部分或全部也会被存储回外部内存，以便将来再次使用其内容。在许多应用程序中，逻辑缓冲区在渲染开始时被清除。如果是这样，加载或存储这些缓冲区的工作就是浪费的。 即使没有这个扩展，如果渲染的一帧从全屏清除开始，OpenGL ES 实现也可以优化掉在渲染帧之前加载帧缓冲区内容的步骤。有了这个扩展，应用程序可以使用 DiscardFramebufferEXT 来表示帧缓冲区的内容将不再需要。在这种情况下，OpenGL ES 实现也可以优化掉在渲染帧后存储帧缓冲区内容的步骤。 glDiscardFramebufferEXT的工作是告知驱动程序你不关心framebuffer的内容。什么驱动程序(或GPU)决定用它做什么 - 这不取决于你。驱动程序可以将所有内容重置为0，或者它可以保持原样，或者当您下次调用glClear时它将使用此信息并且将更有效地执行它(例如通过为内容分配新内存，而不是执行memset与0值)。不要担心它会做什么"
  },"/blog/jekyll/2019-02-03-interrupte.html": {
    "title": "硬中断",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2019-02-03-interrupte.html",
    "body": "【转帖】 本来想写内核如何接收一个网络包这个过程，但发现把整个过程捋顺了，还是很难的。 推导整个过程的起点是中断，包括硬中断和软中断。 而这个过程要是讲清楚吧，感觉在整个网络包接收原理的大流程中有点喧宾夺主。但要是一笔带过吧，那对于这块有困惑的人就很难受，一切的起点没整明白在心里总是个疙瘩。所以，单拎出来一个主题中断，给大家把这个问题搞明白了。 另外，整个操作系统就是一个中断驱动的死循环，操作系统原理如果用一行代码解释，下面这样再合适不过了。 while(true) { doNothing(); } 其他所有事情都是由操作系统提前注册的中断机制和其对应的中断处理函数完成，我们点击一下鼠标，敲击一下键盘，执行一个程序，都是用中断的方式来通知操作系统帮我们处理这些事件，当没有任何需要操作系统处理的事件时，它就乖乖停在死循环里不出来。 所以，中断，非常重要，它也是理解整个操作系统的根基，掌握它，不亏！ 那我们开始吧。 五花八门的中断分类 关于中断的分类，教科书上和网上有很多”标准”答案了，如果你用搜索引擎去寻找答案，可能会找出很多不一样的分类结果。 所以我打算直接在 Intel 手册上找个最官方的标准答案。 在 Intel 手册 Volume 1 Chapter 6.4 Interrupts and Exception 给出。 翻译过来就是，中断可以分为中断和异常，异常又可以分为故障、陷阱、中止。 第一句话有点奇怪，啥叫中断分为中断和异常呢？你看好多文章的时候也是这么写的，不知道你有没有曾疑惑过。 但其实原文的意思准确说是，CPU 提供了两种中断程序执行的机制，中断和异常。第一个中断是个动词，第二个中断才是真正的机制种类。 好吧，我感觉原文也挺奇怪的，但人家就这么叫，没辙。 接下来我只需要翻译一下就好了，再夹杂点自己的解读。 An interrupt is an asynchronous event that is typically triggered by an I/O device. 先说第一个机制中断（interrupt），中断是一个异步事件，通常由 IO 设备触发。比如点击一下鼠标、敲击一下键盘等。 An exception is a synchronous event that is generated when the processor detects one or more predefined conditions while executing an instruction. 再说第二个机制异常（exception），异常是一个同步事件，是 CPU 在执行指令时检测到的反常条件。比如除法异常、错误指令异常，缺页异常等。 这两个机制，殊途同归，都是让 CPU 收到一个中断号，至于 CPU 收到这个中断号之后干嘛，我们暂且不管。 我们先看看收到中断号之前，具体就是中断和异常到底是怎么做到给 CPU 一个中断号的。 先说中断，别眨眼。 有一个设备叫做可编程中断控制器，它有很多的 IRQ 引脚线，接入了一堆能发出中断请求的硬件设备，当这些硬件设备给 IRQ 引脚线发一个信号时，由于可编程中断控制器提前被设置好了 IRQ 与中断号的对应关系，所以就转化成了对应的中断号，把这个中断号存储在自己的一个端口上，然后给 CPU 的 INTR 引脚发送一个信号，CPU 收到 INTR 引脚信号后去刚刚的那个端口读取到这个中断号的值。 估计你被绕晕了，但读我的文章有个好处，太复杂就上动图，来吧。 ![[image-20230320094930185.png]] 你看，最终的目标，就是让 CPU 知道，有中断了，并且也知道中断号是多少。 比如上图中按下了键盘，最终到 CPU 那里的反应就是，得到了一个中断号 0x21。 那异常的机制就更简单了，是 CPU 自己执行指令时检测到的一些反常情况，然后自己给自己一个中断号即可，无需外界给。 比如 CPU 执行到了一个无效的指令，则自己给自己一个中断号 0x06，这个中断号是 Intel 的 CPU 提前就规定好写死了的硬布线逻辑。 好了，到目前为止，我们知道了无论是中断还是异常，最终都是通过各种方式，让 CPU 得到一个中断号。只不过中断是通过外部设备给 CPU 的 INTR 引脚发信号，异常是 CPU 自己执行指令的时候发现特殊情况触发的，自己给自己一个中断号。 还有一种方式可以给到 CPU 一个中断号，但 Intel 手册写在了后面，Chapter 6.4.4 INT n，就是大名鼎鼎的 INT 指令。 ![[image-20230320095004648.png]] INT 指令后面跟一个数字，就相当于直接用指令的形式，告诉 CPU 一个中断号。 比如 INT 0x80，就是告诉 CPU 中断号是 0x80。Linux 内核提供的系统调用，就是用了 INT 0x80 这种指令。 那我们上面的图又丰富了起来。 ![[image-20230320095040034.png]] 有的地方喜欢把他们做一些区分，把 INT n 这种方式叫做软件中断，因为他是由软件程序主动触发的。相应的把上面的中断和异常叫做硬件中断，因为他们都是硬件自动触发的。 但我觉得大可不必，一共就这么几个分类，干嘛还要增加一层理解的成本呢，记三个方式不好么？ 好了，总结一下，给 CPU 一个中断号有三种方式，而这也是中断分类的依据。 1. 通过中断控制器给 CPU 的 INTR 引脚发送信号，并且允许 CPU 从中断控制器的一个端口上读取中断号，比如按下键盘的一个按键，最终会给到 CPU 一个 0x21 中断号。 2. CPU 执行某条指令发现了异常，会自己触发并给自己一个中断号，比如执行到了无效指令，CPU 会给自己一个 0x06 的中断号。 3. 执行 INT n 指令，会直接给 CPU 一个中断号 n，比如触发了 Linux 的系统调用，实际上就是执行了 INT 0x80 指令，那么 CPU 收到的就是一个 0x80 中断号。 再往后，CPU 以各种不同的方式收到的这些 0x21 0x06 0x80，都会一视同仁，做同样的后续处理流程，所以从现在开始，前面的事情就不用再管了，这也体现了分层的好处。 收到中断号之后 CPU 干嘛？ 那 CPU 收到中断号后，如何处理呢？ 先用一句不太准确的话总结，CPU 收到一个中断号 n 后，会去中断向量表中寻找第 n 个中断描述符，从中断描述符中找到中断处理程序的地址，然后跳过去执行。 为什么说不准确呢？因为从中断描述符中找到的，并不直接是程序的地址，而是段选择子和段内偏移地址。然后段选择子又会去全局描述符表中寻找段描述符，从中取出段基址。之后段基址 + 段内偏移地址，才是最终处理程序的入口地址。 当然这个入口地址，还不是最终的物理地址，如果开启了分页，又要经历分页机制的转换，就像下面这样。 不过不要担心，这不是中断的主流程，因为分段机制和分页机制是所有地址转换过程的必经之路，并不是中断这个流程所特有的。 所以我们简单的把中断描述符表中存储的地址，直接当做 CPU 可以跳过去执行的中断处理程序的入口地址，就好了，不影响理解他们。 你看，这是不是简单很多。 那接下来的问题就很简单了，这里出现了两个名词，那就分别对他们进行发问。 1. 中断描述符表是啥？ 2. 中断描述符是啥？ 3. 去哪里找他们？ 分别回答即可 中断描述符表是啥？ 就是一个在内存中的数组而已，操作系统初始化过程中，有很多结构都称之为 XXX 表，其实就是个数组罢了。 以 linux-2.6.0 源码为例，就很直观了。 struct desc_struct idt_table[256] = { {0, 0}, }; 你看，是一个大小为 256 的数组。idt_table 这个名字就是 Interrupt Descriptor Table，逐字翻译过来确实就是中断描述符表。 中断描述符是啥？ 就是中断描述符表这个数组里的存储的数据结构，通过刚刚的源码也可以看出来，是一个叫 desc_struct 的结构。 struct desc_struct {    unsigned long a,b;}; 好家伙，Linux 源码里就这么简单粗暴表示，一个中断描述符的大小为 64 位，也就是 8 个字节，具体里面存的啥通过这个源码看不出来。 翻一下 Intel 手册，在 Volumn 3 Chapter 5.11 IDT Descriptors 中找到了一张图。 可以看到，中断描述符具体还分成好几个种类，有： Task Gate：任务门描述符 Interrupt Gate：中断门描述符 Trap Gate：陷阱门描述符 不要慌，其中任务门描述符 Linux 中几乎没有用到。 中断门描述符和陷阱门描述符的区别仅仅是是否允许中断嵌套，实现方式非常简单粗暴，就是 CPU 如果收到的中断号对应的是一个中断门描述符，就修改 IF 标志位（就是一个寄存器中一位的值），修改了这个值后就屏蔽了中断，也就防止了中断的嵌套。而陷阱门没有改这个标志位，也就允许了中断的嵌套。 所以简单理解的话，你把他们当做同样一个描述符就好了，先别管这些细节，他们的结构几乎完全一样，只是差了一个类型标识罢了。 那这个中断描述符的结构长什么样呢？我们可以清晰地看到，里面有段选择子和段内偏移地址。 回顾下刚刚说的中断处理流程。 没骗你吧。 但以上这些如果你都搞不明白，还是那句话，记这个最简单的流程就好了，不影响理解。 好了，现在我们直观地看到了中断描述符表这个 256 大小的数组，以及它里面存的中断描述符长什么样子，最终的目的，还是帮助 CPU 找到一个程序的入口地址，然后跳转过去。 OK，下一个问题，就是 CPU 怎么寻找到这个中断描述符表的位置呢？它是在内存中一个固定的位置么？ CPU 怎么找到中断描述符表 答案是否定的，中断描述符表在哪里，全凭各个操作系统的喜好，想放在哪里就放在哪里，但需要通过某种方式告诉 CPU，即可。 怎么告诉呢？CPU 提前预留了一个寄存器叫 IDTR 寄存器，这里面存放的就是中断描述符表的起始地址，以及中断描述符表的大小。 在 Volumn 3 Chapter 5.10 Interrupt Descriptor Table 中告诉了我们 IDTR 寄存器的结构。 操作系统的代码可以通过 LIDT 指令，将中断描述符表的地址放在这个寄存器里。 还记得刚刚看的源码么？中断描述符表就是这个。 struct desc_struct idt_table[256] = { {0, 0}, }; 然后操作系统把这个的地址用 LIDT 指令放在 IDTR 寄存器就行了。IDTR 寄存器里的值一共 48 位，前 16 位是中断描述符表大小（字节数），后 32 位是中断描述符表的起始内存地址，就是这个 idt_table 的位置。 Linux-2.6.0 源码中是这样构造这个结构的，简单粗暴。 idt_descr:    .word 256 * 8 - 1    .long idt_table 紧接着，一个 LIDT 指令把这个结构放到 IDTR 寄存器中。 lidt idt_descr 整个过程一气呵成，呵得我连代码格式都懒得调了，是不是很清晰明了。 这样，CPU 收到一个中断号后，中断描述符表的起始位置从 IDTR 寄存器中可以知道，而且里面的每个中断描述符都是 64 位大小，也就是 8 个字节，那自然就可以找到这个中断号对应的中断描述符。 接下来的问题就是，这个中断描述符表是谁来提前写好的？又是怎么写的？ 谁把中断描述符表这个结构写在内存的 很简单，操作系统呗。 在 Linux-2.6.0 内核源码的 traps.c 文件中，有这样一段代码。 void __init trap_init(void) {    set_trap_gate(0, &amp;divide_error);    ...    set_trap_gate(6, &amp;invalid_op);    ...    set_intr_gate(14, &amp;page_fault);    ...    set_system_gate(0x80, &amp;system_call);} 你看，我们刚刚提到的除法异常、非法指令异常、缺页异常，以及之后可能通过 INT 0x80 触发系统调用的中断处理函数 system_call，就是这样被写到了中断描述符表里。 经过这样一番操作后，我们的中断描述符表里的值就丰富了起来。 好了，现在只剩下最后一个问题了，CPU 在找到一个中断描述符后，如何跳过去执行？ 找到中断描述符后，干嘛 现在这个问题可以再问得大一些了，就是 CPU 在收到一个中断号并且找到了中断描述符之后，究竟做了哪些事？ 当然，最简单的办法就是，直接把中断描述符里的中断程序地址取出来，放在自己的 CS:IP 寄存器中，因为这里存的值就是下一跳指令的地址，只要放进去了，到下一个 CPU 指令周期时，就会去那里继续执行了。 但 CPU 并没有这样简单粗暴，而是帮助我们程序员做了好多额外的事情，这增加了我们的学习和理解成本，但方便了写操作系统的程序员，拿到一些中断的信息，以及中断程序结束后的返回工作。 但其实，就是做了一些压栈操作。 1. 如果发生了特权级转移，压入之前的堆栈段寄存器 SS 及栈顶指针 ESP 保存到栈中，并将堆栈切换为 TSS 中的堆栈。 2. 压入标志寄存器 EFLAGS。 3. 压入之前的代码段寄存器 CS 和指令寄存器 EIP，相当于压入返回地址。 4. 如果此中断有错误码的，压入错误码 ERROR_CODE 5. 结束（之后就跳转到中断程序了） 压栈操作结束后，栈就变成了这个样子。 特权级的转移需要切换栈，所以提前将之前的栈指针压入。错误码可以方便中断处理程序做一些工作，如果需要，从栈顶拿到就好了。 抛开这两者不说，剩下的就只有标志寄存器和中断发生前的代码地址，被压入了栈，这很好理解，就是方便中断程序结束后，返回原来的代码嘛~ 具体的压栈工作，以及如何利用这些栈的信息达到结束中断并返回原程序的效果，Intel 手册中也写得很清楚。 Volumn 3A System Programming Guide  Chapter 5.12.1Exception- or Interrupt-Handler Procedures 看下面的话，通过配合 IRET 或 IRETD 指令返回。 由于后续版本的 Linux 自己的玩法比较多，已经不用 Intel 提供的现成指令了，所以这回我们从 Linux-0.11 版源码中寻找答案。 比如除法异常的中断处理函数，在 asm.s 中。 _divide_error: push dword ptr _do_divide_error ;no_error_code: ; xchg [esp],eax ; push ebx push ecx push edx push edi push esi push ebp push ds ; push es push fs push 0 ; lea edx,[esp+44] ; push edx mov edx,10h ; mov ds,dx mov es,dx mov fs,dx call eax ; add esp,8 ; pop fs pop es pop ds pop ebp pop esi pop edi pop edx pop ecx pop ebx pop eax ;// 弹出原来eax 中的内容。 iretd 只看最后一行，确实用了 iretd 指令。 这个指令会依次弹出栈顶的三个元素，把它们分别赋值给 EIP，CS 和 EFLAGS，而栈顶的三个元素，又恰好是 EIP，CS 和 EFLAGS 这样的顺序，你说这巧不巧？ 当然不巧，人家 CPU 执行中断函数前做了压栈操作，然后又提供了 iret 指令做弹栈操作，当然是给你配套使用的！ 你看，中断是如何切到中断处理程序的？就是靠中断描述符表中记录的地址。那中断又如何回到原来的代码继续执行呢？是通过 CPU 帮我们把中断发生前的地址压入了栈中，然后我们程序自己利用他们去返回，当然也可以不返回。 这就是 CPU 和操作系统配合的结果，把中断这个事给解决了。 总结 所以总结起来就是，理解中断，只要回答了这几个问题就好。 如何给 CPU 一个中断号？ 外部设备通过 INTR 引脚，或者 CPU 执行指令的过程中自己触发，或者由软件通过 INT n 指令强行触发。 同样中断也是这样进行分类的。 CPU 收到中断号后如何寻找到中断程序的入口地址？ 通过 IDTR 寄存器找到中断描述符表，通过中断描述符表和中断号定位到中断描述符，取出中断描述符表中存储的程序入口地址。 中断描述符表是谁写的？ 操作系统代码写上去的。 找到程序入口地址之后，CPU 做了什么？ 简单说，实际上做的事情就是压栈，并跳转到入口地址处执行代码。而压栈的目的，就是保护现场（原来的程序地址、原来的程序堆栈、原来的标志位）和传递信息（错误码） 好了，中断讲完了，如果再往后扩大一点点概念，以上说的中断，统统都是硬中断。注意，不叫硬件中断哦。 为什么叫硬中断呢？因为这是 Intel CPU 这个硬件实现的中断机制，注意这里是实现机制，并不是触发机制，因为触发可以通过外部硬件，也可以通过软件的 INT 指令。 那与硬中断对应的还有软中断，这个概念网上好多地方都讲错了，把软中断和 INT 指令这种软件中断混淆了，所以我觉得软件中断最好称其为，由软件触发的中断，而软中断称其为软件实现的中断。 软中断是纯粹由软件实现的一种类似中断的机制，实际上它就是模仿硬件，在内存中有一个地方存储着软中断的标志位，然后由内核的一个线程不断轮询这些标志位，如果有哪个标志位有效，则再去另一个地方寻找这个软中断对应的中断处理程序。 软中断是 Linux 实现中断的下半部的一种非常常见的方式，之后我讲 Linux 内核如何接受网络包这个事情的时候也可以看到，软中断是研究整个过程的一个突破口。 EOF - 推荐阅读  点击标题可跳转 1、10 分钟看懂 Docker 和 K8S 2、这才是中国被卡脖子最严重的软件！ 3、如果让你来设计网络，你会把它弄成啥样？ 看完本文有收获？请分享给更多人 推荐关注「Linux 爱好者」，提升Linux技能 Linux爱好者 点击获取《每天一个Linux命令》系列和精选Linux技术资源。「Linux爱好者」日常分享 Linux/Unix 相关内容，包括：工具资源、使用技巧、课程书籍等。 75篇原创内容 公众号 点赞和在看就是最大的支持❤️ 分享收藏 微信扫一扫 关注该公众号"
  },"/blog/jekyll/2019-01-11-Weston.html": {
    "title": "Weston",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2019-01-11-Weston.html",
    "body": "1. 启动Weston 1.1 实现内容 解析cmdline 初始化log系统 创建wl_display对象，并侦听client接入 创建weston_compositor对象，从而创建global resource compositor 和 shm， 以前其他资源 load backend， 默认为drm_backend, 在drm_backend初始化的过程中会load gl_renderer load shell, 默认为desktop-shell.so 调用wl_display_run( ) 循环等待event的发生 1.2 伪代码 int wet_main(int argc, char *argv[], const struct weston_testsuite_data *test_data){ // 初始化 layoutput_list wl_list_init(&amp;wet.layoutput_list); ... // parse command line ... // init log system ... // 调用wayland提供的函数wl_display_create() // 创建 wl_display 对象 display = wl_display_create(); ... // 创建 weston_compositor 对象 // --&gt; 创建 global resource compositor and shm 以及其他resource weston_compositor_create( ); // 读取config for compositor ... // load backend，通用的为load_drm_backend( ) // 最终调用 对应backend实现的weston_backend_init( ) // 在backend 初始化的过程中会装载 gl_renderer load_backend(compositor, backend) ... // 创建socket， 侦听client的连接请求 weston_create_listening_socket(display, socket_name) ... // load shell, 默认为 desktop-shell.so wet_load_shell(compositor, shell, ...) ... // loop, 循环等待event的发生 wl_display_run(display) ... // 退出流程，资源的释放 } 1.3 backend, renderer, shell的作用 结构图 backend { destroy() repaint_begin() //composite之前调用 repaint_cancel() // 中途取消 repaint_flush() // composite 完成后调用， 可用于实现提交到display create_output() // 创建weston_output device_changed() can_scanout_dmabuf() }Weston_backend, Compositor-&gt;backend renderer: renderer接口供backend内部使用，外部通过调用backend接口触发 { display_create() output_window_create() output_pbuffer_create() output_destroy() output_set_border() create_fence_fd() } gl_renderer_interface 2. Client 动作 2.1 Client的接入和global资源代理的创建 流程 连接display 获得registry，注册listener，用于处理weston资源变化时的callback 根据资源变化的callback， 创建各类资源的proxy 进入loop，不断调用wl_display_dispatch( )，使得wayland内部循环处理各类event 伪代码 static void global_resource_found(void* data, struct wl_registry* registry, uint32_t name, const char* interface, uint32_t version) { // 通过字符串interface 判断是什么resource， // 通过wl_registry_bind() 创建对应的 resource proxy if (strcmp(interface, \"wl_compositor\") == 0) { // 构建了 compositor 的 proxy compositor = wl_registry_bind(registry, name, &amp;wl_compositor_interface, 4)); }else if (strcmp(interface, \"wl_shm\") == 0) { // 构建了 shm 的proxy shm = wl_registry_bind(registry, name, &amp;wl_shm_interface, 1); } ... } // 当weston的global resoure发生变化时，通过如下回调函数通知到client wl_registry_listener registry_listener = { // 发现新global resource的回调函数 global_resource_found, // global resource remove的回调函数 global_resource_remove } int main(int argc, char** argv) { // 1. 调用wayland提供的 wl_display_connect( ), 连接到weston( wayland server) // 对应到weston启动中的weston_create_listening_socket() display = wl_display_connect(NULL); ... // 2. 获取 wl_registry, 并侦听它的callback registry= wl_display_get_registry(display); wl_registry_add_listener(registry, &amp;registry_listener, display); ... // 3. 循环等待， // 调用wl_display_dispatch( )，由wayland处理weston发来的event while(ret != -1){ ret = wl_display_dispatch( ); } } 2.2 内部weston - client 通讯机制 request: Client –&gt; Server event : Server –&gt; Client 术语上，Wayland 中把 Client 发给 Server 的跨进程函数调用称为 request，反方向的跨进程函数调用称为 event。 本质上，它们处理的方式是类似的。 要让两个进程通过 socket 进行函数调用，首先需要将调用抽象成数据流的形式。这个数据流应该包含函数名、参数等信息。 RPC 函数的接口定义是应该同时包含在 Client 和 Server 端的库中的，其中包含了接口对象所支持的 request 和 event 的函数签名。因此这部分不用传输，只要传输目标对象 id，方法 id 和参数列表这些信息就可以了。 这些信息会通过 wl_closure_marshal()写入 wl_closure 结构，再由 serialize_closure()变成数据流。 等到了目标进程后， 会从数据流通过 wl_connection_demarshal()转回 wl_closure。 RPC 图示 object RPC 机制 这个过程类似于 Android 中的 Parcel 机制。那么 问题来了，参数中的整形，字符串什么的都好搞，拷贝就行。但如果参数中包含对象，我们不能把整个对象 拷贝过去，也不能传引用过去。那么需要一种机制来作同一对象在 Server 和 Client 端的映射，这是通过 wl_map 实现的。 wl_map 在 Client 和 Server 端各有一个，它们分别存了 wl_proxy 和 wl_resource 的数组，且是 一一对应的。这些对象在这个数组中的索引作为它们的 id。这样，参数中的对象只要传 id，这个 id 被传到目 的地后会通过查找这个 wl_map 表来得到本地相应的对象。在功能上类似于 Android 中的 BpXXX 和 BnXXX。 wl_proxy 和 wl_resource 都包含 wl_object 对象。这个 wl_object 和面向对象语言里的对象概念类似，它有 interface 成员描述了这个对象所实现的接口，implementation 是这些接口的实现函数的函数指针数组，id 就是 在 wl_map 结构里数组中的索引。 前面所说的 Client 绑定 Server 端资源的过程就是在 Client 端创建 wl_proxy， 在 Server 端创建 wl_resource。然后 Client 就可以通过 wl_proxy 调用 Server 端对应 wl_resource 的 request， Server 端就可以通过 wl_resource 调用 Client 端对应 wl_proxy 的 event。 这个映射过程如下图所示(以 wl_registry 为例) 2.3 Client 创建各类资源proxy // wl_surface wl_surface = wl_compositor_create_surface(compositor) // wl_buffer wl_shm_pool = wl_shm_create_pool( ) wl_buffer = wl_shm_pool_create_buffer( ) // attach buffer to surface wl_surface_attach(wl_surface, wl_buffer) /* 以下与窗口的管理\\显示相关 */ // xdg_surface // xdg_wm_base 它也是一个global resouce，对应到 desktop-shell xdg_surface = xdg_wm_base_get_xdg_surface(xdg_wm_base, wl_surface) // xdg_toplevel xdg_toplevel = xdg_surface_get_toplevel(xdg_toplevel) // wl_keyboard // wl_seat 是一个global resource // 通过wl_keyboard 创建一个listener就可接收按键 wl_keyboard = wl_seat_get_keyboard(wl_seat) wl_keyboard_add_listener(wl_keyboard, keyboard_listener) // wl_pointer 鼠标指针 // 通过wl_pointer 创建一个listener可以接收鼠标的移动信息 wl_pointre = wl_seat_get_pointer(wl_weat) wl_pointer_add_listener（wl_pointer, pointer_listener) ... ... 2.4 Client 渲染 伪代码 simple-egl.c /*-------------------- egl 初始化工作 ------------------------*/ // egl lib 应该要支持wayland。 // 这样在调用一些egl接口时，在其内部会调用wayland接口与Wayland server交换信息 // 如函数：eglGetDisplay( ) , eglCreateWindowSurface( )， eglSwapBuffers 等 // 1. 获取egl_display egl_display = weston_platform_get_egl_display(EGL_PLATFORM_WAYLAND_KHR, wl_display, ...) or egl_display = eglGetDisplay(wl_display) // 2. 初始化 egl eglInitialize(egl_display) // 3. 通用elg 配置 eglGetConfigs() eglChooseConfig( ) eglCreateContext( ) /* ----------------------gl 准备工作------------------------------*/ // 1.创建 shader glCreateShader( ) // 2.创建 Program glCreateProgram( ) // 3. attach shader to program glAttachShader( ) // 4. Link program glLinkProgram( ) // 5. 使用program glUseProgram( ) /*-------------------------wl_surface 关联egl_surface----------------*/ wl_egl_window-&gt;surface = wl_surface; wl_egl_window-&gt;width = width; wl_egl_window-&gt;height = height; eglCreateWindowSurface(egl_display, ... , wl_egl_window) /*------------------------------------------------------------------*/ gl 绘制图形 /*-------------------------------------------------------------------*/ // 内部实现应该调用wayland接口来swap buffer eglSwapBuffers(egl_display, egl_surface) ... 2.5. Client 提交渲染好的surface wl_surface_commit( ) 3. 各个Surface的合成与呈现呈现· 3.1 流程 compositor遍历每个weston_output 发起repaint。weston_output_schedule_repaint( ) 通知weston_output具体实现–backend_output, 开始repaint的前期准备工作, 对应函数start_repaint_loop( )， drm实现暂无内容 backend_output通知compositor可以开始output repaint compositor 调用weston_output相关backend的repaint_begin( )，drm_backend 创建了pending_state compoistor 调用weston_output_repaint(), 开始repaint。 调用weston_compositor_build_view_list( ) 构建view_list, 得到output的一个paint_node_z_order_list 调用drm_backend assign_planes( ) 设置输出plane 调用drm_backend drm_output_repaint( ), 最终指向gl_renderer_repaint_output( ) 依据paint_node_z_order_list, OpenGL依次建立shader，texture等进行渲染 全部完成后，提交呈现 sequenceDiagram participant C as Compositor participant O as Weston_output participant B as Backend loop 遍历weston_output_list C -&gt;&gt; O: 要开始repaint_loop &lt;br /&gt;call backend_output start_repaint_loop( ) C -&gt;&gt; B: repaint_begin( ) Note right of B: 创建 pending_state C -&gt;&gt; O: 通知output repaint O -&gt;&gt; C: 构建view_list, build_view_list( ) Note left of C:构建paint_node_z_order_list C -&gt;&gt; B: assign_planes( ),设置输出plane C -&gt;&gt; B: drm_output_repaint( ) Note right of B: 调用OpenGL API &lt;br/&gt;结合paint_node_z_order_list&lt;br/&gt;进行渲染 end"
  },"/blog/linux/2018-08-01-DeviceResourceManage.html": {
    "title": "设备资源管理模块",
    "keywords": "linux",
    "url": "/blog/linux/2018-08-01-DeviceResourceManage.html",
    "body": "1.解决的问题 相信每一个写过Linux driver的工程师，都在probe函数中遇到过上面的困惑：在顺序申请多种资源（IRQ、Clock、memory、regions、ioremap、dma、等等）的过程中，只要任意一种资源申请失败，就要回滚释放之前申请的所有资源。 于是在函数的最后，就一定会出现很多的goto标签，用于释放不同的资源（如上面的exit_free_irq、exit_free_dma、等等）。 在申请资源出错时，小心翼翼的goto到正确的标签上，以便释放已申请资源。 这样在代码中，整个函数被大段的、重复的如下代码充斥。 if (!condition) { err = xxx; goto xxx; } 既浪费精力容易出错，也不美观。 有困惑，就有改善的办法。 方法就是Linux设备模型中的device resource management（设备资源管理）。 2.解决的思路 devres提供了一种机制，用资源节点的形式记录它申请的资源，并在系统中为设备分配一个链表，当申请某个资源时，就构建一个资源节点，然后把它加入到这个链表中，对应的释放函数也会被记录，以便在driver detach的时候，自动释放。 为了使用devres机制，资源要对各自的资源分配函数重新封装，加入资源节点的申请、添加和释放，一般新函数名改成了devm_xxx()的形式。driver作者只管调用这些devm_xxx()接口来申请资源，不用考虑释放，设备模型会在适当的时候释放它们。 device resource management位于“drivers/base/devres.c”中，它实现了上述机制。 3.提供的接口 以下是devres提供的几个基本接口 interface Description devres_alloc( ) // 分配资源节点 devres_free( ) // 释放资源节点 devres_add( ) // 添加资源节点到链表 devres_destroy( ) // 释放资源 devres_release_all() //释放所有资源 4.接口的使用 其他资源模块，可以通过调用devres提供的接口，利用devres机制实现资源的自动释放。 4.1 资源节点函数的应用举例 下面的代码是利用devres机制实现分配中断资源函数 devm_request_threaded_irq( ), 上层模块可以调用它来分配中断资源，在出错时，不必考虑对该资源的释放，系统会自动释放。 主要涉及到devres_alloc()、devres_free()和devres_add() 4.2 资源释放函数的应用举例 资源释放函数devres_destroy()的使用举例，资源模块可以用它来封装资源释放函数。 5. 函数的内部实现 5.1 devres_alloc() devrs_alloc()函数的实现，主要调用了内部函数alloc_dr(), 它会分配size+sizeof(struct devres)的内存大小, struct devres用于存储资源节点信息，并记录release 函数。 5.2 devres_add() devres_add()主要实现把资源节点添加到设备的资源链表中。 5.3 devres_destroy() devres_destroy()主要涉及到以下几个内部函数： devres_remove() //查找到资源节点，并从链表中删除 find_dr() //根据release函数指针、match函数查找资源节点 devres_free() //释放资源节点 可以结合上面它的使用实例来学习。 5.4 devers_release_all() devers_release_all()的调用会释放所有资源。它的被调用时机有两个： really_probe()失败 设备与驱动分离时, deriver_dettach时 就是driver_remove时。"
  },"/blog/jekyll/2018-05-20-memory_check.html": {
    "title": "进程内存检查",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2018-05-20-memory_check.html",
    "body": "1.进程内存映射文件smaps 在内核的数据结构中，进程、进程使用内存、虚拟内存块和一个二进制程序文件的对应关系图如下。 查看/proc/${PID}/smaps，可以得到每一个vm_area_node的详细信息。 下图是一个具体的vm_area_node信息。 1.1 两种映射 下面两种映射的介绍，是为了下一节解释各字段含义做准备。 文件映射 就是存储介质(比如：磁盘)中的数据通过文件系统映射到内存再通过文件映射映射到虚拟空间，这样，用户就可以在用户空间通过 open, read, write 等函数区操作文件内容。代码中函数open(), read(), write(), close(), mmap(fd，…)… 操作的虚拟地址都属于文件映射。 匿名映射 就是用户空间要求内核分配一定的物理内存来存储数据，这部分内存不属于任何文件。内核就使用匿名映射将内存中的某段物理地址与用户空间一一映射，这样用户就可用直接操作虚拟地址来范围这段物理内存。比如使用malloc(), mmap(NULL，…)申请内存。 1.2 各字段含义 第一行 08048000-080bc000: 该虚拟内存段的开始和结束位置 r-xp:内存段的权限，分别是可读、可写、可运行、私有或共享，最后一位p代表私有，s代表共享(如共享的内存， shm). 如果有”w”，表示是库的数据区. 00000000: 虚拟内存段起始地址在对应的映射文件中以页为单位的偏移量， 对匿名映射，它等于0或者vm_start/PAGE_SIZE 03:02: 文件的主设备号和次设备号。 对有名映射来说，是映射的文件所在设备的设备号 对匿名映射来说，因为没有文件在磁盘上，所以没有设备号，始终为00:00。 13130: 被映射到虚拟内存的文件的索引节点号,通过该节点可以找到对应的文件， 对匿名映射来说，因为没有文件在磁盘上，所以为0 /bin/bash: 被映射到虚拟内存的文件名称。后面带(deleted)的是内存数据，可以被销毁。 对有名映射来说，是映射的文件名。 对匿名映射来说，是此段虚拟内存在进程中的角色。[stack]表示在进程中作为栈使用，[heap]表示堆。其余情况比如mmap(NULL, ….)则无显示。 Size 虚拟内存空间大小。但是这个内存值不一定是物理内存实际分配的大小，因为在用户态上，虚拟内存总是延迟分配的。这个值计算也非常简单，就是该VMA的开 始位置减结束位置。 延迟分配:就是当进程申请内存的时候，Linux会给他先分配页，但是并不会区建立页与页框的映射关系，也就是并不会分配物理内存，而当真正使用的时候，就会产生一个缺页异常，硬件跳转page fault处理程序执行，在其中分配物理内存，然后修改页表(创建页表项)。异常处理完毕，返回程序用户态，继续执行。 Rss resident set size 实际分配的内存，这部分物理内存已经分配，不需要缺页中断就可以使用的。但可能是和其他进程共享的。 这里有一个公式计算Rss： Rss=Shared_Clean+Shared_Dirty+Private_Clean+Private_Dirty Shared_Clean Shared_Dirty Private_Clean Private_Dirty share/private：表示该页面是共享还是私有。 dirty/clean： 表示该页面是否被修改过，如果修改过（dirty），在页面被淘汰的时候，就会把该脏页面回写到交换分区(换出，swap out)。有 一个标志位用于表示页面是否dirty。 share/private_dirty/clean 计算逻辑： 查看该page的引用数，如果引用&gt;1，则归为shared，如果是1，则归为private，再查看该page的flag，是否标记为_PAGE_DIRTY，如果不是，则认为干净的 Pss proportional set size 平摊计算后的实际物理使用内存(有些内存会和其他进程共享，例如mmap进来的)。实际上包含上面private_clean+private_dirty，和按比例均分的shared_clean、shared_dirty。 举个计算Pss的例子： 如果进程A有x个private_clean页面，有y个private_dirty页面，有z个shared_clean仅和进程B共享，有h个shared_dirty页面和进程B、C共享。那么进程A的Pss为：x + y + z/2 + h/3 Referenced 当前页面被标记为已引用或者包含匿名映射（The amount of memory currently marked as referenced or a mapping associated with a file may contain anonymous pages）。在Linux内存管理的页面替换算法中，当某个页面被访问后，Referenced标志被设置，如果该标志设置了，就不能将该页移出。 Anonymous 匿名映射的物理内存，这部分内存不是来自于文件。 VmFlags vm_area的各种属性，具体如下： 1.3 不同变量的位置 一个库映射到内存， 一般分为代码段、数据段和只读数据段 r- --p: so中的字符串常数 rw--p: so中的全局变量，静态变量 r- -xp: so的代码段，常量 - ---p: 表示该 VMA 是私有的，不可执行，且不可读写. 这通常用于保护敏感数据或代码，防止其被修改或执行 下面这段代码展示了不同变量的存储位置： 2.free 命令 free 命令用于显示系统的内存状态，包括物理内存、交换内存（swap）和内核缓冲区内存。详细输出如下： Mem 行（第二行）显示了内存的使用情况。 Swap行（第三行）显示了交换空间的使用情况。 total: 表示系统总的可用物理内存和交换空间大小。 used : 表示已经被使用的物理内存和交换空间。 free : 表示还有多少物理内存和交换空间可用使用。 shared: 显示被共享使用的物理内存大小。 buff/cache: 显示被 buffer 和 cache 使用的物理内存大小。 available: 显示还可以被应用程序使用的物理内存大小。 2.1 buffer与cache buffer: 缓冲区 CPU 在进行一系列操作时，先在内存的一块区域进行，一系列操作完成后，再一次性把该内存区域提交给外部设备，来对这个区域操作。 比如写一堆数据给硬盘，就先写到内存的一块区域，写好后一次写回到硬盘。又比如读数据，先在内存划出一块区域，让硬盘控制器写数据到这块区域，写好后，CPU 直接访问该区域得到数据。这个内存区域就叫buffer 缓冲区是内存或存储的一部分，用于在等待从输入设备传输到输出设备时存放项目。 操作系统通常在打印文档时使用缓冲区。这个过程称为排队（spooling），它将要打印的文档发送到缓冲区，而不是立即发送到打印机。如果打印机没有自己的内部存储器，或者内存已满，操作系统的缓冲区会保存等待打印的信息，同时打印机以自己的速度从缓冲区打印。 通过将文档排队到缓冲区，处理器可以继续解释和执行指令，同时打印机进行打印。这使用户可以在打印机打印时继续在计算机上进行其他任务。多个打印作业在缓冲区中排队（发音为“Q”）。一个名为打印排队程序（print spooler）的程序拦截操作系统中要打印的文档，并将其放入队列中 cache：缓存 CPU 要访问一块数据时，首先访问内存的某个区域，看是否有该数据的缓存，有则直接访问，没有则访问它的来源地。 CPU 利用内存或高速缓存对数据的再备份，为以后的再次访问提供方便 缓存如今的大多数计算机通过缓存（发音为“cash”）来提高处理速度。 缓存有两种主要类型：内存缓存和磁盘缓存。让我们详细了解一下内存缓存。 L1 缓存： L1 缓存直接内置在处理器芯片中。 它通常容量很小，范围从 8 KB 到 128 KB。 L1 缓存存储经常使用的指令和数据，以便快速访问。 L2 缓存： L2 缓存比 L1 缓存稍慢，但容量更大。 它的大小范围从 64 KB 到 16 MB。 一些现代处理器包括高级传输缓存，这是一种直接内置在处理器芯片上的 L2 缓存类型。 使用高级传输缓存的处理器的性能比不使用它的处理器要快得多。 现今的个人计算机通常具有 512 KB 到 12 MB 的高级传输缓存。 缓存通过存储经常使用的指令和数据来显著加快处理时间。 当处理器需要一条指令或数据时，它按照以下顺序搜索内存：L1 缓存，然后是 L2 缓存，然后是 RAM。 如果所需信息在内存中找不到，处理器必须搜索速度较慢的存储介质，例如硬盘或光盘。 2.2. 手动释放缓存 首先，使用sync命令将未写入磁盘的数据同步到磁盘，以确保文件系统的完整性。 然后，通过设置/proc/sys/vm/drop_caches来释放内存缓存： echo 1 &gt; /proc/sys/vm/drop_caches：释放页缓存。 echo 2 &gt; /proc/sys/vm/drop_caches：释放 dentries 和 inodes。 echo 3 &gt; /proc/sys/vm/drop_caches：释放所有缓存。 3.mtrace mtrace 是 Linux 系统内核自带的一个内存追踪函数。它会在每个内存申请函数（malloc、realloc、calloc）的位置记录下信息，并在每个内存释放的位置记录下 free 的内存信息。其中包含有内存申请的地址、内存申请的大小、释放内存的地址、释放内存的大小。 具体来说，mtrace 函数的作用如下： 安装钩子函数，用于跟踪内存分配和释放。 记录有关内存分配和释放的跟踪信息。 可以用于发现程序中的内存泄漏和试图释放未分配内存的情况。 使用方式： 在代码中包含 &lt;mcheck.h&gt; 头文件。 在程序启动时调用 mtrace() 函数，开启内存分配和释放跟踪。 程序结束时，可以调用 muntrace() 函数关闭内存分配和释放跟踪。 运行mtrace脚本，分析跟踪日志，生成报告。 请注意，mtrace 的跟踪输出通常是文本形式，不一定易于人类阅读。GNU C 库提供了一个 Perl 脚本 mtrace，用于解析跟踪日志并生成人类可读的输出。为了获得最佳效果，建议编译时启用调试，以便在可执行文件中记录行号信息。不过，mtrace 的跟踪会带来性能损耗.如果 MALLOC_TRACE 没有指向有效且可写的路径， 则mtrace不会记录信息。 4.strace与ltrace ltrace 用于跟踪程序的库函数调用，而 strace 则用于跟踪系统调用。 它们都基于 ptrace 系统调用，但跟踪库函数和跟踪系统调用之间存在差异。 通过它们，我们也可以对应用的内存申请、释放进行跟踪。 ltrace 的工作原理： ptrace 附加到正在运行的程序。 定位程序的 PLT。 使用 PTRACE_POKETEXT 设置软件断点（int $3 指令）覆盖库函数的 PLT 中的汇编 trampoline。 恢复程序执行。 strace 应该也是相类似的工作原理 4.1 strace strace 是一个强大的 Linux 命令，用于诊断、调试和统计。它允许您跟踪正在运行的程序的系统调用和接收的信号。下面是一些关于 strace 的参数使用方法。 -c：统计每个系统调用的执行时间、次数和错误次数。 示例：打印执行 uptime 时系统调用的时间、次数和错误次数： strace -c uptime -f：跟踪子进程，这些子进程是由当前跟踪的进程创建的。 -i：在系统调用时打印指令指针。 -t：跟踪的每一行都以时间为前缀。 -tt：如果给出两次，则打印时间将包括微秒。 -ttt：如果给定三次，则打印时间将包括微秒，并且前导部分将打印为自启动以来的秒数。 -T：显示花费在系统调用上的时间。 限定表达式： -e trace=set：仅跟踪指定的系统调用集。例如，trace=open,close,read,write 表示仅跟踪这四个系统调用。 -e trace=file：跟踪所有以文件名作为参数的系统调用。示例：打印执行 ls 时与文件有关的系统调用： strace -e trace=file ls -e trace=process：跟踪涉及进程管理的所有系统调用。 -e trace=network：跟踪所有与网络相关的系统调用。 -e trace=signal：跟踪所有与信号相关的系统调用。 -e trace=ipc：跟踪所有与 IPC 相关的系统调用。 -e trace=memory：跟踪所有与 momory 相关的系统调用。 其他参数： -o 文件名：将跟踪输出写入文件而不是 stderr。 -p pid：使用进程 ID pid 附加到该进程并开始跟踪 下面是运行 strace ls 的输出 4.2 ltrace ltrace 是一个用于跟踪程序库调用的 Linux 工具。它可以拦截并记录被执行进程调用的动态库函数，以及该进程接收到的信号。此外，ltrace 还可以拦截并打印程序执行的系统调用。 常用参数和示例： -c：统计每个系统调用的执行时间、次数和错误次数。 示例：打印执行 uptime 时系统调用的时间、次数和错误次数： ltrace -c uptime -f：跟踪子进程，这些子进程是由当前跟踪的进程创建的。 -i：在系统调用时打印指令指针。 -t：跟踪的每一行都以时间为前缀。 -tt：如果给出两次，则打印时间将包括微秒。 -ttt：如果给定三次，则打印时间将包括微秒，并且前导部分将打印为自启动以来的秒数。 -T：显示花费在系统调用上的时间。 限定表达式： -e trace=set：仅跟踪指定的系统调用集。例如，trace=open,close,read,write 表示仅跟踪这四个系统调用。 -e trace=file：跟踪所有以文件名作为参数的系统调用。示例：打印执行 ls 时与文件有关的系统调用： ltrace -e trace=file ls -e trace=process：跟踪涉及进程管理的所有系统调用。 -e trace=network：跟踪所有与网络相关的系统调用。 -e trace=signal：跟踪所有与信号相关的系统调用。 -e trace=ipc：跟踪所有与 IPC 相关的系统调用。 其他参数： -o 文件名：将跟踪输出写入文件而不是 stderr。 -p pid：使用进程 ID pid 附加到该进程并开始跟踪。 下面是运行 ltrace ls 的输出"
  },"/blog/jekyll/2018-05-15-Wayland&Weston.html": {
    "title": "Wayland&amp;Weston",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2018-05-15-Wayland&Weston.html",
    "body": "1.编译 预设环境 $export WLD=“~/xxxxxxxxx” //定义一个wayland目录，编译生成到这里 sudo apt install meson sudo apt install some_depenced_libs Wayland $ git clone https://gitlab.freedesktop.org/wayland/wayland.git $ cd wayland $ meson build/ --prefix=$WLD $ ninja -C build/ install $ cd .. Wayland protocols $ git clone https://gitlab.freedesktop.org/wayland/wayland-protocols.git $ cd wayland-protocols $ meson build/ --prefix=$WLD $ ninja -C build/ install $ cd .. Weston $ git clone https://gitlab.freedesktop.org/wayland/weston.git $ cd weston $ meson build/ --prefix=$WLD $ ninja -C build/ install $ cd .. 第三方依赖库的编译 需要的第三方库 Libxml2-dev Libexpat-dev Libffi-dev Libinput-dev Libdrm-dev Libxkbcommon-dev libpixman-1-dev libcairo2-dev libudev 它们有的通过configure配置然后make， 有的通过mesa, ninja编译. 需要注意设置正确的PKG_CONFIG_PATH，使的pkg-config可以找到相关的库信息。 如果是mesa，留意目录下是否有meson_options.txt, 其中定义了各种编译选项，可对它进行修改。 meson 交叉编译 在meson系统中进行交叉编译，需要为meson中添加参数 –cross-file xxx_filename, 在xxx_filename中定义gcc等的路径路径 下面是一个示例 [binaries] c = '/opt/cross-arm/bin/arm-linux-gnueabihf-gcc' cpp = '/opt/cross-arm/bin/arm-linux-gnueabihf-g++' ld ='/opt/cross-arm/bin/arm-linux-gnueabihf-ld' strip= '/opt/cross-arm/bin/arm-linux-gnueabihf-strip' pkgconfig ='/usr/bin/pkg-config' [host_machine] system = 'linux' cpu_family = 'aarch64' cpu = 'cortex-a73' endian = 'little 在configure系统中，则是export GCC CFLAG 等环境变量 指定输入输出 meson build_dir/ sourc_code_dir/ 2.运行 configure file copy weston.ini to ~/.config/ run weston. login as root ./weston –tty=2 Ctrl+Alt+Backspace –&gt; 退出Weston界面 run weston-client test login as root export WAYLAND_DISPLAY=wayland-1 示例1, 指定backend 和shell Server: ./weston --tty=2 --shell=fullscreen-shell.so --backend= drm-backend.so Client: weston-simple-dmabuf-egl 示例2 Server：./weston --tty=2 --shell=fullscreen-shell.so --backend=fbdev-backend.so Client./weston-simple-damage 3.Wayland 3.1 code的组成 wayland主要由三部分组成。 Wayland提供了protocol的定义方式 在路径protocol文件夹下，以xml的形式定义了Wayland的核心协议。 如下面的xml，就定义了wl_display &lt;interface name=\"wl_display\" version=\"1\"&gt; &lt;description summary=\"core global object\"&gt; The core global object. This is a special singleton object. It is used for internal Wayland protocol features. &lt;/description&gt; ..... &lt;/interface&gt; xml到code的转换工具 xml到code的转换工具是wayland-scanner，它的source code在wayland目录下，可通过编译生成它。 wayland还实现了一个高效率的 Server+Client通信模式 Server端，主要是使用epoll+socket监听Client端事件，并对收到的消息反序列化。 Client端：wayland-client提供了已实现的序列化接口 总之，官方提供的Wayland源码，主要包括协议的定义、协议到代码的生成工具，以及一套实现好的通信模型 3.2 基本对象 几乎所有的Wayland API都需要Wayland全局对象作为参数。 名称 作用 wl_display 表示与服务器的连接。 wl_registry 全局对象注册表，全局对象需要通过它获取。 wl_compositor 窗口合成器，也是服务器。 wl_shm 内存管理器，与窗口合成器共享内存用。 wl_shell 支持窗口操作功能。 wl_seat 输入设备管理器。 wl_pointer 代表鼠标设备。 wl_keyboard 代表键盘设备。 Wayland没有提供Get函数来获取以上全局对象，只能通过wl_registry获取全局对象 3.2.1 Client端object的使用 Wayland中server提供给client使用的对象可以归为 global object和 resource object。 Global 也是一中resource。 Global object如 wl_display, wl_compositor, wl_seat 等。 它们在client端是通过bind来获取到一个client 对应对象，然后就可以对它进行操作 Resource object 在client端是通过Global object 来创建的， 如 wl_surface, wl_shell_surface 等。 3.2.2 Server端object的实现 Global 资源的创建： wl_global_create( …., bind_xxx_function ) bind_xxx_function // 在client中调用wl_registry_bind函数时被call wl_resource_create( ) // wayland protocal 提供的接口， 登记资源 wl_resource_set_implementation( ) // 设定资源接口的实现 普通 Resource 的创建： 应该是由某些 对global 资源的操作触发 wl_resource_create() wl_resource_set_implementation( ) 3.2.3 Client Server通讯 Listener 是server –&gt; client 的通知方式. Client 注册listener 给server，有监听事件发生，server发信息给client. Server侧发生通知的函数命名特征: xxxx_send_xxxx( ) 3.3 简单的wayland app 流程 3.4 Wayland log 需设置环境变量： export WAYLAND_DEBUG=1 http://127.0.0.1:4000/blog/assets/wayland/image-20230209140244854.png 4.Weston 简介 Weston是基于wayland协议，实现的Compositor。 Weston的入口在（这里以10.0.0为例）: weston-10.0.0./compositor/main.c weston-10.0.0./compositor/executable.c 4.1 框架与对象 框架 Weston中有以下几个主要部分：Shell、Compositor、Render、backend、Input Shell：窗口管理器，画面层级、窗口信息、窗口生命周期、Focus窗口等等一些偏向于业务层的处理。默认的shell为desktop-shell，同时提供了其他shell实现（如ivi-shell） Compositor：负责画面的合成，使用DRM连接output，将画面输出到实际显示设备。 Render：负责渲染，比如gl-render，做一些纹理贴图操作。 Input：libinput模块，与evdev、uvdev模块交互，从底层设备节点接收touch、key等输入 backend: Weston 使用后端的概念来抽象其运行环境的底层接口。后端负责处理输入和生成输出 对象 4.2 main函数的基本框架 实现内容： 解析cmdline 初始化log系统 创建wl_display对象，并侦听client接入 创建weston_compositor对象，从而创建global resource compositor 和 shm， 以前其他资源 load backend， 默认为drm_backend, 在drm_backend初始化的过程中会load gl_renderer load shell, 默认为desktop-shell.so 调用wl_display_run( ) 循环等待event的发生 int wet_main(int argc, char *argv[], const struct weston_testsuite_data *test_data){ // 初始化 layoutput_list wl_list_init(&amp;wet.layoutput_list); ... // parse command line ... // init log system ... // 调用wayland提供的函数wl_display_create() // 创建 wl_display 对象 display = wl_display_create(); ... // 创建 weston_compositor 对象 // --&gt; 创建 global resource compositor and shm 以及其他resource weston_compositor_create( ); // 读取config for compositor ... // load backend，通用的为load_drm_backend( ) // 最终调用 对应backend实现的weston_backend_init( ) // 在backend 初始化的过程中会装载 gl_renderer load_backend(compositor, backend) ... // 创建socket， 侦听client的连接请求 weston_create_listening_socket(display, socket_name) ... // load shell, 默认为 desktop-shell.so wet_load_shell(compositor, shell, ...) ... // loop, 循环等待event的发生 wl_display_run(display) ... // 退出流程，资源的释放 } 4.3 backend创建 Weston 使用后端的概念来抽象其运行环境的底层接口。后端负责处理输入和生成输出。 作为 libweston 的用户，Weston 可以在不同的后端上运行，包括嵌套在其他 Wayland 合成器中的方式（使用 wayland 后端），也可以在 X11 上运行，或者在独立的后端上运行，例如 DRM/KMS。 大多数情况下，人们应该允许 Weston 自动选择后端，因为它会产生最佳结果。例如，在已经运行另一个图形环境的机器上运行 Weston 时，它会自动选择合适的后端，无论是另一个 Wayland 合成器还是 X11 服务器。 只有在你知道 Weston 自动选择的后端不是最佳选择，或者你想使用不同于默认加载的后端时，才需要手动指定后端。在这种情况下，可以使用 -B [backend] 命令行选项来选择后端。 可用的后端包括： drm：独立运行在 DRM/KMS 和 evdev 上（推荐）。 wayland：作为 Wayland 应用程序嵌套在另一个 Wayland 合成器实例中。 x11：作为 X11 应用程序嵌套在 X11 显示服务器实例中。 rdp：作为一个没有本地输入或输出的 RDP 服务器运行。 headless：无输入或输出运行，适用于测试套件。 pipewire：无输入，输出到 PipeWire 节点。 weston_backend_init { destroy() repaint_begin() //composite之前调用 repaint_cancel() // 中途取消 repaint_flush() // composite 完成后调用， 可用于实现提交到display create_output() // 创建weston_output device_changed() can_scanout_dmabuf() }Weston_backend, Compositor-&gt;backend 4.4 shell创建 weston提供了多种shell，比如：desktop shell，ivi-shell, kiosk shell, fullscreen shell等 4.5 head和output weston_head 和 weston_output 是libweston中的两个关键概念，用于管理显示输出和图像呈现。 weston_head（头部） weston_head 表示一个连接器或监视器。 在硬件驱动中，头部通常指的是一个显示器，但它也可以是另一个窗口系统中的窗口，或者是一个虚拟概念。 头部是一个可以呈现图像的位置。 weston_head 负责以下任务： 管理帧缓冲区。 跟踪损坏区域。 处理显示时序。 管理重绘状态机。 在显示硬件中，weston_head 表示一个CRTC（显示控制器），但仅在成功启用后才会如此。在头部的生命周期内，CRTC 可能会切换到另一个。 weston_head 的生命周期由libweston用户控制。 您可以通过将至少一个weston_head 附加到weston_output 来构建一个可供合成器使用的weston_output 对象，然后使用 weston_output_enable() 启用该输出。已启用的输出无法重新配置，但这在未来可能会发生变化。您可以使用 weston_output_disable() 来禁用一个输出，然后重新配置它，但这会导致可见的故障。 weston_output（输出） weston_output 决定了全局合成器坐标空间的哪一部分将被合成成图像以及何时进行合成。 该图像在附加的头部上呈现。 weston_output 负责以下任务： 帧缓冲区管理。 损坏区域跟踪。 显示时序。 重绘状态机。 视频模式、输出比例和输出变换是输出的属性。 在显示硬件中，weston_output 表示一个CRTC，但仅在成功启用后才会如此。CRTC 可能会在输出的生命周期内切换到另一个。 weston_output 的生命周期由libweston用户控制。 下面是各种case的流程图 Heads are being created on compositor start-up with a backend that manages head lifetimes completely on its own A compositor handles libweston notification of something with heads having changed. This happens on both compositor start-up and later due to hotplug A compositor creates and configures an output for a head or heads it wants to light up. A compositor finds out a head has been disconnected and proceeds to destroy the corresponding output. The backend realises that a piece of hardware has disappeared and needs to destroy the corresponding head. The head is released, and even when the compositor is not listening for head destroy signal, the output gets automatically disabled, though not destroyed. 4.6 layer, view, surface 4.7 create_surface 4.8 get_shell_surface 4.9 create_pool 4.10 surface_attach surface_damage 4.11 surface_commit 4.12 create_shm_buffer 4.13 weston event loop 4.14 weston idle 处理 4.15 gl-renderer 4.16 libinput 为了提高输入管理部分的模块性，Weston将对输入设备(键盘，鼠标，触摸屏等)的处理分离到一个单独的库，也就是libinput 中。具体地，它提供了设备检测，设备处理，输入事件处理等基本功能，类似于Android 中的EventHub。此外它还有pointer acceleration, touchpad support 及gesture recognition等功能。 libinput更像是一个框架，它将几个更底层的库的功能整合起来。它主要依赖于以下几个库: mtdev: Multi-touch 设备处理，比如它会将不带tracking ID的protocol A转化为 protocol B。 libevdev: 与kernel中evdev 模块对接。 libudev:主要用于和 udevd的通信，从而获取设备的增加删除事件。也可从kernel获取。 Weston 中的输入管理模块与libinput对接，它实现了两大部分的功能: 对输入设备的维护， 对输入事件的处理。 对于输入事件既会在Weston中做处理，也会传给相应的 client。 从事件处理模型上来看，libinput主循环监听udev monitor fd，它主要用于监听设备的添加删除事件。如果有设备添加，会打开该设备并把fd加入到libinput的主循环上。另一方面，Weston中会将 libinput 的 epoll fd加入主循环。这样形成级联的epoll，无论是 udev monitor 还是input device的fd有事件来，都会通知到Weston和libinput的主循环。 Weston中支持三种输入设备，分别是键盘，触摸和鼠标。一套输入设备属于一个seat(严格来说，seat中包括一套输入输出设备)。因此，weston_seat 中包含weston_keyboard,weston_pointer 和weston_touch三个结构。系统中可以有多个seat,它们的结构被串在weston_compositor 的 seat_list链表中。 可以看到，对于焦点处理，每个设备有自己的focus,它指向焦点窗口，用于拖拽和输入等。成员focus_resource_list 中包含了焦点窗口所在client中输入设备 proxy对应的 resource 对象。在这个 list 中意味着可以接收到相应的事件。 4.17 Client创建窗口 4.17.1 创建shm窗口 4.17.2 创建egl窗口 4.18 调用的backtrace 4.18.1 Client eglSwapBuffers Client App 调用eglSwapBuffers 提交buffer给compositor的堆栈 4.18.2 wl_output global的创建 4.18.3 wl_surface_commit wl_surface_commit() 触发的后继操作 4.18.4 drm_output_repaint() drm_output_repaint() 的调用栈 4.18.5 repaint_views repaint_views() 的调用栈 4.18.6 第一次repaint的触发 4.18.7 送显的backtrace 4.18.8 weston-desktop-shell 4.18.9 Weston_keyboard 进程的创建 在weston.ini中的【input-method】设置 path= 空 来不创建它, 函数launch_input_method() 会检查path. 4.18.10 key的处理 4.19 weston misc 4.19.1 定时器函数 创建定时器：wl_event_loop_add_timer() Enable 定时器：wl_event_source_timer_update( source, ms_delay) // ms_delay ==0 disable 4.19.2 wayland signal wl_signal_add( , ) 添加一个listener到 listerner_list 链表 wl_signal_emit( , ) 触发一个signal, 从listener_list中调用每个listener notify 4 12.3 libwayland-egl.so.xxx 由 wayland-1.20.0/egl下文件编译生成 主要功能生成/销毁wl_egl_window， 并获取它的大小属性 使用: client/backend-wayland 可以利用它来生成wl_egl_window, 然后传给eglCreateWindowSurface( ) 示例：./clients/simple-egl.c 好像不需要再分配buffer attch到 wl_surface, 参考simple-egl-window.c 4.19.4 weston_client_start() 在weston里定义， Weston call 它来 发起一个client 进程, 它会调用weston_client_launch() 4.19.5 weston-screenshooter 截屏进程， 被desktop-shell 进程call screenshooter_create( )创建 4.19.6 显示一帧的过程 epoll收到event 构建compositor View_list， 含有order信息 backend call gl_renderer set current surface 依次根据各个view的信息构建纹理，进行渲染 eglswapbuffer（） 遍历各个output，完成1～4 repaint_flash() -&gt; drm 上屏 4.19.7 repaint_timer_triger 4.19.8 Compositor sleep 相关 In weston.init, 相关配置 idle-time， 单位sec 在函数weston_compositor_wake()设置 多长时间无操作进入sleep状态 定时器compositor-&gt;idle_source， 在weston_compositor_create()中创建 在weston_compositor_offscreen() , weston_compositor_sleep() 中关闭定时器 4.19.9 surface与buffer Surface： Surface 是 DRM 中的一个概念，用于描述一个可绘制的区域。它是一个抽象的图形表面，可以用于绘制图像、文本或其他内容。 Surface 可以是屏幕上的一部分，也可以是一个窗口、一个图像或其他可视元素。 应用程序可以将图形绘制到 Surface 上，然后由 DRM 管理其显示。 例如，在 DRM 中，一个窗口可以有多个关联的 Surface，每个 Surface 对应一个缓冲区。 Buffer： Buffer 是一块内存区域，用于存储像素数据。在 DRM 中，它通常与 Surface 关联。 Buffer 可以是帧缓冲区、纹理、渲染缓冲区等。 Buffer 存储着图像的像素值，可以直接访问和操作。 例如，当应用程序绘制图像时，它将像素数据写入 Buffer，然后由 DRM 将其显示在屏幕上。 总结： Surface 是一个抽象的图形表面，用于绘制图像。 Buffer 是实际存储像素数据的内存区域，与 Surface 关联。 4.19.10 drm_virtual_output drm_backend_init_virtual_output_api () &lt;- #ifdef BUILD_DRM_VIRTUAL &lt;- /libweston/backend-drm/meson.build &lt;- remoting or pipewire in configure drm_virtual_output 用于 remoting or pipewire 场景， 在meson_options.txt 里配置 4.19.11 explicit-synchronization"
  },"/blog/jekyll/2018-03-21-YUV.html": {
    "title": "YUV编码",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2018-03-21-YUV.html",
    "body": "1.简介 YUV数据由Y、U、V三个分量组成，现在通常说的YUV指的是YCbCr。 Y：表示亮度（Luminance、Luma），占8bit（1字节） Cb、Cr：表示色度（Chrominance、Chroma） Cb（U）：蓝色色度分量，占8bit（1字节） Cr（V）：红色色度分量，占8bit（1字节） 2.采样方式(444, 422, 420的区别) 把Y、U、V数据转变为R、G、B时用到 2.1 采样方式 采样方式通常用A:B:C的形式来表示，比如4:4:4、4:2:2、4:2:0等 A：假定在一块A*2个像素的概念区域，一般都是4. B：第1行的色度(UV)采样数目。 C：第2行的色度(UV)采样数目 所以这里的B,C指的分别是在第一行，第二行UV采样的数目。 C的值一般要么等于B，要么等于0 示意图1： 示意图2 上图中，不管是哪种采样格式，Y分量都是全水平、全垂直分辨率采样的，每一个像素都有自己独立的Y分量 2.2 占用字节数 由上可以推算出不同采样方式下每个像素需要的平均字节数。 4:4:4 一个像素YUV各占一个字节，总共3个字节 24bit 4:2:2 8个像素 ： 8个Y + 2个U +2个V +2个U +2个V = 16字节 每个像素：16字节/8 = 2个字节 16bit 4:2:0 8个像素 ： 8个Y + 2个U +2个V = 12字节 每个像素：12字节/8 = 1.5个字节 12bit 3.存储方式(Planar, Semi-Planar和Packed的区别) 存储格式，表示的是Y、U、V数据是如何排列和存储的。 读取或写入Y、U、V数据时用到. 3.1 分类 YUV的存储格式可以分为3大类： 名称 特点 Planar(平面) Y、U、V分量分开单独存储,名称通常以字母p结尾, 3个planar Semi-Planar（半平面） Y分量单独存储，U、V分量交错存储, 名称通常以字母sp结尾, 1个planar Packed（紧凑） 或者叫Interleaved(交错), Y、U、V分量交错存储, 1个planar 3.2 444 I444 和YV24 主要是UV次序的不同 semi-planar NV24 和NV42 主要是UV交替次序的不同 3.3 422 Planar I422 YV16 区别：VU 次序 Semi-Planar NV16 NV61 区别：VU 次序 Packed UYVY YUYV YVYU 区别：VU 次序 3.4 420 Planar I420 YV12 采样方式420 I420，像素示意图 Semi-Planar NV12 NV21 采样方式420，各种存储方式， 像素示意图 4.借助ffmpeg格式转换 PNG -&gt; YUV ffmpeg -i in.png -s 512x512 -pix_fmt yuv420p out.yuv YUV -&gt; PNG ffmpeg -s 512x512 -pix_fmt yuv420p -i in.yuv out.jpg YUV 文件只是存储数据的文件，没有大小信息，所以转换时一定要给出它的尺寸 PNG 文件含有尺寸信息，所以转YUV时可以不指定大小，默认原大小 5.借助ffplay显示YUV 可以通过ffplay显示YUV数据。 YUV中直接存储的是所有像素的颜色信息（可以理解为是图像的一种原始数据） 必须得设置YUV的尺寸（-s）、像素格式（-pix_fmt）才能正常显示 ffplay -s 512x512 -pix_fmt yuv420p in.yuv # 在ffplay中 # -s已经过期，建议改为：-video_size # -pix_fmt已经过期，建议改为：-pixel_format ffplay -video_size 512x512 -pixel_format yuv420p in.yuv 6.GLSL实现YUV转RGBA 6.1 基本计算公式 根据的标准不同，有不同的计算公式。下面是一个可以在shader中使用的计算方法。 只要先得到Y,U,V， 就可以按下面方法转换RGB \" yuv.y = yuv.y - 0.5; \\n\" \" yuv.z = yuv.z - 0.5; \\n\" \" \\n\" \" rgb.r = yuv.x + 1.402 * yuv.z; \\n\" \" rgb.g = yuv.x - 0.34413 * yuv.y - 0.71414 * yuv.z; \\n\" \" rgb.b = yuv.x + 1.772 * yuv.y; \\n\" YUV到RGBA的转换其实就两个要点 构建合适的纹理 在shader中提前YUV 6.2 YUV444P-&gt;ARGB 6.2.1 构建纹理 static GLuint build_texture_4_yuv444p(int width, int height, void *data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_RED, width, height, 0, GL_RED, GL_UNSIGNED_BYTE, data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 6.2.2 在shader中提取YUV static const char* yuv444_2_rgba_frag_src = \"uniform sampler2D Sampler; \\n\" \"varying highp vec2 TexCoord; \\n\" \"void main (void) \\n\" \"{ \\n\" \" highp vec3 yuv; \\n\" \" highp vec3 rgb; \\n\" \" yuv.x = texture2D(Sampler, TexCoord).r; \\n\" \" yuv.y = texture2D(Sampler, vec2(TexCoord.x, TexCoord.y+0.3333333)).r;\\n\" \" yuv.z = texture2D(Sampler, vec2(TexCoord.x, TexCoord.y+0.6666667)).r;\\n\" \" \\n\" \" yuv.y = yuv.y - 0.5; \\n\" \" yuv.z = yuv.z - 0.5; \\n\" // 矩阵计算方法 \" rgb = mat3( \\n\" \"1, 1, 1, \\n\" \"0, -.34413, 1.772, \\n\" \"1.402, -.71414, 0 \\n\" \" ) * yuv; \\n\" \" \\n\" \" gl_FragColor = vec4(rgb, 1.0); \\n\" \"} \\n\"; 6.3 NV24-&gt;ARGB 6.3.1 构建纹理 这里需要构建两个纹理，一个是Y的纹理， 一个是UV的纹理 static GLuint build_texture_4_nv24_y(int width, int height, void *y_data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width, height, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, y_data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } static GLuint build_texture_4_nv24_uv(int width, int height, void *uv_data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE_ALPHA, width, height, 0, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, uv_data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 6.3.2 在shader中提取YUV static const char* nv24_2_rgba_frag_src = \"uniform sampler2D Sampler_y; \\n\" \"uniform sampler2D Sampler_uv; \\n\" \"varying highp vec2 TexCoord; \\n\" \"void main (void) \\n\" \"{ \\n\" \" highp vec3 yuv; \\n\" \" highp vec3 rgb; \\n\" \" \\n\" \" yuv.x = texture2D(Sampler_y, TexCoord).r; \\n\" \" yuv.y = texture2D(Sampler_uv, TexCoord).r; \\n\" \" yuv.z = texture2D(Sampler_uv, TexCoord).a; \\n\" \" \\n\" \" yuv.y = yuv.y - 0.5; \\n\" \" yuv.z = yuv.z - 0.5; \\n\" \" \\n\" \" rgb.r = yuv.x + 1.402 * yuv.z; \\n\" \" rgb.g = yuv.x - 0.34413 * yuv.y - 0.71414 * yuv.z; \\n\" \" rgb.b = yuv.x + 1.772 * yuv.y; \\n\" \" \\n\" \" gl_FragColor = vec4(rgb, 1.0); \\n\" \"} \\n\"; 6.4 NV16-&gt;ARGB 6.4.1 构建纹理 这里需要构建两个纹理，一个是Y的纹理， 一个是UV的纹理 static GLuint build_texture_4_nv16_y(int width, int height, void *data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width, height, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 注意glTexImage2D()的第四参数值发生了变化 static GLuint build_texture_4_nv16_uv(int width, int height, void *data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE_ALPHA, width/2, height, 0, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 6.4.2 在shader中提取YUV 与NV24方法相同 6.5 NV12-&gt;ARGB 6.5.1 构建纹理 这里需要构建两个纹理，一个是Y的纹理， 一个是UV的纹理 static GLuint build_texture_4_nv12_y(int width, int height, void *data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width, height, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, data); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 注意glTexImage2D()的第四,五参数值发生了变化 static GLuint build_texture_4_nv12_uv(int width, int height, void *data){ GLuint texture; glGenTextures(1, &amp;texture); glBindTexture(GL_TEXTURE_2D, texture); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); CHK_GL_ERR(); glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE_ALPHA, width/2, height/2, 0, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, data); CHK_GL_ERR(); glBindTexture(GL_TEXTURE_2D, 0); return texture; } 6.5.2 在shader中提取YUV 与NV24方法相同"
  },"/blog/drm/2018-02-01-DRM.html": {
    "title": "DRM子系统",
    "keywords": "DRM",
    "url": "/blog/drm/2018-02-01-DRM.html",
    "body": "1.DRM总述 1.1在整个系统中的位置 App角度 DRM内部 wayland实例 2.图形buffer相关 2.1 dumb buffer 旧时的显卡由一块很小的显存（通常为640x480）加一块数模转换电路（DAC）组成，说白了就是一块 Framebuffer + Display Controller。显卡的功能极其简单，只负责将显存中的图像数据转换成RGB信号发送出去即可，而所有的绘图操作则都交给 CPU 来完成。行业里将这种显卡称为 VGA(Video Graphics Array) Card，它的显存则被称为“Dumb Frame Buffer”。而到了后期，随着显卡技术的不断发展，许多原来由 CPU 干的活，渐渐的都被显卡取代了。从最初支持某些特定绘图指令（如画点、画线）的显卡，到后来支持视频解码的 Video Card，再到现代支持复杂3D渲染指令（如OpenGL）的 GPU 显卡，CPU 绘图的繁重任务彻底得到了解放。与 VGA Card 相比较，行业里将后来显卡的显存称为“Smart Frame Buffer”。 首先从这两种称谓上我们就可以看出，dumb 是 smart 的反义词，因此 dumb 在这里的解释应该是“傻的”或“傻瓜式的”，而不是“哑的”。 dumb buffer 和 smart buffer 的区别就在于，你写到显存里的数据，是可以直接上屏显示的图像内容，还是一堆需要GPU解析的命令和资源数据。 与 dumb buffer 命名类似的还有： dumb-terminal：不支持特殊字符的终端，如“清屏”、“粗体”、“彩色字符”等等 dumb-panel：不带 GRAM 的 panel dumb-TV：与 Smart-TV 相反，指以前老式的黑白电视 如今的 IT 领域，dumb一词更多的代表 “功能简单的”、“老式的”、“传统的” 含义 2.1.1超简单DUMB实现 与dumb buffer相关的userspace接口有四个, 下表列出了它们的名字, 作用和对应要实现的函数 interface Description drm driver ioctl(fd, DRM_IOCTL_MODE_CREATE_DUMB, …) 向驱动申请一个dumb buffer，返回一个handle，指向新分配的buffer .dumb_create() ioctl(fd, DRM_IOCTL_MODE_MAP_DUMB, …) 为buffer map做准备，传入handle，得到一个offset .dumb_map_offset() ioctl(fd, DRM_IOCTL_MODE_DESTROY_DUMB, …) 销毁该dumb buffer .dumb_destroy() mmap(fd, …) 传入 buffer对应的offset，映射到进程空间，返回一个用户空间可使用的地址。 .fops.mmap() 2.1.1.1 简单应用程序 下面是一个简单的应用程序，演示了dumb buffer的申请，mmap，使用和销毁。 #include &lt;fcntl.h&gt; #include &lt;stdio.h&gt; #include &lt;sys/mman.h&gt; #include &lt;unistd.h&gt; #include &lt;xf86drm.h&gt; #define log(fmt, args...) printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define err(fmt, args...) printf(\"\\033[35m%s():%d \" fmt \"\\033[0m\\n\", __func__, __LINE__, ##args) static int create_dumb_buffer(int fd, int width, int height){ int ret = 0; struct drm_mode_create_dumb create = {}; create.width = width; create.height =height; create.bpp = 4*8; // byte per pixel ret = drmIoctl(fd, DRM_IOCTL_MODE_CREATE_DUMB, &amp;create); if(ret){ err(\"ret:%d\", ret); return 0; } return create.handle; } static int get_dumb_buffer_offset(int fd, int handle){ struct drm_mode_map_dumb map = {}; map.handle = handle; int ret = drmIoctl(fd, DRM_IOCTL_MODE_MAP_DUMB, &amp;map); if(ret){ err(\"ret:%d\", ret); return 0; } return map.offset; } static void* mmap_dumb_buffer(int fd, int size, int offset){ void *addr = NULL; addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, offset); if(!addr){ err(\"mmap dumb buffer offset:0x%x failed\", offset); return NULL; } return addr; } static void destroy_dumb_buffer(int fd, int handle){ struct drm_mode_destroy_dumb destroy = {}; destroy.handle = handle; int ret = drmIoctl(fd, DRM_IOCTL_MODE_DESTROY_DUMB, &amp;destroy); if(ret){ err(\"DRM_IOCTL_MODE_DESTROY_DUMB failed. ret:%d\", ret); return; } } int main(int argc, char **argv){ int fd; int width = 1024; int height = 1; int size = width*height; fd = open(\"/dev/dri/card0\", O_RDWR | O_CLOEXEC); if(fd&lt;=0){ err(\"open dev failed.\"); return -1; } int handle = create_dumb_buffer(fd, width, height); log(\"create dumb buffer, get handle:%d\", handle); int offset = get_dumb_buffer_offset(fd, handle); log(\"get dumb buffer offset:0x%x\", offset); void *addr = mmap_dumb_buffer(fd, size, offset); log(\"get dumb buffer addr:%p\",addr); //在addr所指向的buffer上进行绘图 memset(addr, 0, width*height); munmap(addr, size); destroy_dumb_buffer(fd, handle); log(\"destroy dumb buffer\"); getchar(); close(fd); return 0; } 2.1.1.2 实现简单的驱动 基于kernel 5.15.126， 主要是.dumb_create(), .dumb_map_offset(), .dumb_destroy, fops.mmap()的实现 #include &lt;linux/module.h&gt; #include &lt;linux/platform_device.h&gt; #include &lt;drm/drm_drv.h&gt; #include &lt;drm/drm_file.h&gt; #include &lt;drm/drm_ioctl.h&gt; #define DRIVER_NAME \"drm dumb driver\" #define DRIVER_DESC \"Virtual drm dumb driver\" #define DRIVER_DATE \"20191114\" #define DRIVER_MAJOR 1 #define DRIVER_MINOR 0 #define MAX_NUM 10 #define log(fmt, args...) printk(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define err(fmt, args...) printk(\"\\033[35m%s():%d \" fmt \"\\033[0m\\n\", __func__, __LINE__, ##args) struct dumb_device { struct drm_device drm; struct platform_device *platform_dev; }; static struct dumb_device* dumb_device = NULL; static struct page *pages[MAX_NUM] = {0}; static u32 buffer_size[MAX_NUM] = {0}; static int page_idx = 0; static int dumb_mmap_impl(struct file* filp, struct vm_area_struct* vma) { unsigned long pfn_start = 0; unsigned long size = vma-&gt;vm_end - vma-&gt;vm_start; int ret = 0; int idx = 0; /********************************************************************* * vm_pgoff: 也是vm_are_struct的一个字段 * 表示偏移量，它是以page size计数的。 pg表示page计数，off表示偏移 *********************************************************************/ idx = vma-&gt;vm_pgoff; pfn_start = page_to_pfn(pages[idx]); log(\"idx:%d phy: 0x%lx, vm_pgoff: 0x%lx, vma-&gt;vm_start:0x%lx, size: 0x%lx\", idx, pfn_start &lt;&lt; PAGE_SHIFT, vma-&gt;vm_pgoff, vma-&gt;vm_start, size); ret = remap_pfn_range(vma, vma-&gt;vm_start, pfn_start, size, vma-&gt;vm_page_prot); if (ret) err(\"remap_pfn_range failed at [0x%lx 0x%lx]\", vma-&gt;vm_start, vma-&gt;vm_end); else{ unsigned long virt_start = (unsigned long)page_address(pages[idx]); log(\"map 0x%lx to 0x%lx, size: 0x%lx\", virt_start, vma-&gt;vm_start, size); } return 0; } static const struct file_operations dumb_driver_fops = { .owner = THIS_MODULE, .open = drm_open, .release = drm_release, .unlocked_ioctl = drm_ioctl, .compat_ioctl = drm_compat_ioctl, .mmap = dumb_mmap_impl, }; static void dumb_release(struct drm_device* dev) { log(); } static int dumb_create_impl(struct drm_file *file_priv, struct drm_device *dev, struct drm_mode_create_dumb* args) { u32 size = 0; log(); if(page_idx&gt;=MAX_NUM){ err(\"only support alloc %d buffers\", MAX_NUM); return -ENOMEM; } size = roundup((args-&gt;width * args-&gt;height * args-&gt;bpp/8), PAGE_SIZE); // 分配页面 pages[page_idx] = alloc_pages(GFP_KERNEL, get_order(size)); if (!pages[page_idx]) { err(\"alloc_pages() failed\"); return -ENOMEM; } buffer_size[page_idx] = size; // 赋值返回参数 args-&gt;size = size; args-&gt;pitch = args-&gt;width * args-&gt;bpp/8; args-&gt;handle = page_idx++; return 0; } static int dumb_map_offset_impl(struct drm_file *file_priv, struct drm_device *dev, uint32_t handle, uint64_t* offset) { log(\"%d\", handle); //根据handle得到一个offset，为了简单，这里直接使用handle *offset = 0x1000*(u64)handle; return 0; } static int dumb_destroy_impl(struct drm_file *file_priv, struct drm_device *dev, uint32_t handle){ log(\"handle:%d\", handle); free_pages((unsigned long)page_address(pages[handle]), get_order(buffer_size[handle])); return 0; } static struct drm_driver dumb_driver = { .fops = &amp;dumb_driver_fops, .release = dumb_release, .dumb_create = dumb_create_impl, .dumb_map_offset = dumb_map_offset_impl, .dumb_destroy = dumb_destroy_impl, .name = DRIVER_NAME, .desc = DRIVER_DESC, .date = DRIVER_DATE, .major = DRIVER_MAJOR, .minor = DRIVER_MINOR, }; static int __init dumb_drv_init(void) { int ret; struct platform_device* pdev = NULL; log(\"build time: %s %s\", __DATE__, __TIME__); pdev = platform_device_register_simple(DRIVER_NAME, -1, NULL, 0); if (IS_ERR(pdev)) return PTR_ERR(pdev); if (!devres_open_group(&amp;pdev-&gt;dev, NULL, GFP_KERNEL)) { ret = -ENOMEM; goto out_unregister; } // 分配 dumb_device = devm_drm_dev_alloc(&amp;pdev-&gt;dev, &amp;dumb_driver, struct dumb_device, drm); if (IS_ERR(dumb_device)) { ret = PTR_ERR(dumb_device); goto out_devres; } // 注册 ret = drm_dev_register(&amp;dumb_device-&gt;drm, 0); if (ret) goto out_devres; dumb_device-&gt;platform_dev = pdev; log(\"Finish\"); return 0; out_devres: devres_release_group(&amp;pdev-&gt;dev, NULL); out_unregister: platform_device_unregister(pdev); return ret; } static void __exit dumb_drv_exit(void) { struct platform_device *pdev = dumb_device-&gt;platform_dev; log(); drm_dev_unregister(&amp;dumb_device-&gt;drm); devres_release_group(&amp;pdev-&gt;dev, NULL); platform_device_unregister(pdev); } module_init(dumb_drv_init); module_exit(dumb_drv_exit); MODULE_AUTHOR(\"kevin\"); MODULE_DESCRIPTION(DRIVER_DESC); MODULE_LICENSE(\"GPL\"); 2.1.1.3 它们是如何衔接起来的 2.1.2 基于GEM的实现 GEM是系统提供的一套实现框架或者帮助工具。在各自实现私有的drm driver时，总有一些内容是相同或相似的，GEM就对这些相同部分进行了总结，抽象，然后实现了它们，供开发者使用。具体实现在drm_gem.c 2.1.2.1 实现代码 下面是基于GEM对上面driver实现的改造。 它利用了GEM 提供的如下函数： interface 功能 drm_gem_object_init() 初始化一个gem对象 drm_gem_handle_create() 通过gem对象生成一个handle drm_gem_object_lookup() 根据handle 找到对应的gem对象 drm_gem_dumb_map_offset() 通过gem对象生成一个offset drm_gem_mmap() 对mmap()的支持，如果开发者使用它，就可利用gem mmap的实现部分 struct drm_gem_object_funcs 使用gem对象时可能调用到一组函数,此处实现了.mmap,对应drm_gem_mmap() #include &lt;linux/module.h&gt; #include &lt;linux/platform_device.h&gt; #include &lt;drm/drm_gem.h&gt; #include &lt;drm/drm_drv.h&gt; #include &lt;drm/drm_file.h&gt; #include &lt;drm/drm_ioctl.h&gt; #define DRIVER_NAME \"drm dumb gem driver\" #define DRIVER_DESC \"Virtual drm dumb driver\" #define DRIVER_DATE \"20191116\" #define DRIVER_MAJOR 1 #define DRIVER_MINOR 0 #define log(fmt, args...) printk(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define err(fmt, args...) printk(\"\\033[35m%s():%d \" fmt \"\\033[0m\\n\", __func__, __LINE__, ##args) #define DRM_GEM_OBJECT_TO_DUMB_BUFFER_OBJ(target) \\ container_of(target, struct dumb_buffer_object, gem_obj) struct dumb_device { struct drm_device drm; struct platform_device *platform_dev; }; struct dumb_buffer_object{ struct drm_gem_object gem_obj; struct page *start_page; u32 size; }; static struct dumb_device* dumb_device = NULL; static int dumb_gem_mmap_impl(struct file* filp, struct vm_area_struct* vma) { log(\"call drm_gem_mmap()\"); // 利用GEM方法进行mapping， // 在它里面会调用之前设置好的drm_gem_object_funcs.mmap() return drm_gem_mmap(filp, vma); } static const struct file_operations dumb_driver_fops = { .owner = THIS_MODULE, .open = drm_open, .release = drm_release, .poll = drm_poll, .read = drm_read, .llseek = noop_llseek, .mmap = dumb_gem_mmap_impl, .unlocked_ioctl = drm_ioctl, .compat_ioctl = drm_compat_ioctl, }; static int dumb_gem_obj_mmap(struct drm_gem_object *gem_obj, struct vm_area_struct *vma){ int ret = 0; unsigned long pfn_start = 0; unsigned long size = vma-&gt;vm_end - vma-&gt;vm_start; struct dumb_buffer_object *dumb_buffer_obj = NULL; dumb_buffer_obj = DRM_GEM_OBJECT_TO_DUMB_BUFFER_OBJ(gem_obj); log(\"mmaping dumb_buffer_obj:%p\", dumb_buffer_obj); pfn_start = page_to_pfn(dumb_buffer_obj-&gt;start_page); ret = remap_pfn_range(vma, vma-&gt;vm_start, pfn_start, size, vma-&gt;vm_page_prot); if (ret){ err(\"remap_pfn_range failed at [0x%lx 0x%lx]\", vma-&gt;vm_start, vma-&gt;vm_end); } return 0; } static const struct drm_gem_object_funcs dumb_gem_obj_funcs = { .open = NULL, .mmap = dumb_gem_obj_mmap, //.vm_ops = &amp;dumb_gem_vm_ops, // 赋值到struct vm_area_struct 的vm_ops, 供缺页时调用。 }; static void dumb_release(struct drm_device* dev) { log(); } static int dumb_create_gem_impl(struct drm_file* file_priv, struct drm_device* dev, struct drm_mode_create_dumb* args) { u32 size = 0; int ret = 0; struct dumb_buffer_object *dumb_buffer_obj; struct drm_gem_object *gem_obj; log(); size = ALIGN((args-&gt;width * args-&gt;height * args-&gt;bpp/8), PAGE_SIZE); //1. 分配buffer_obj // 可以在此处分配page,也可以在fault时再分配 dumb_buffer_obj = kzalloc(sizeof(*dumb_buffer_obj), GFP_KERNEL); size = roundup(size, PAGE_SIZE); log(\"page_size:%d\",size); // 2. 初始化dumb_buffer_obj内的drm_gem_object gem_obj = &amp;(dumb_buffer_obj-&gt;gem_obj); gem_obj-&gt;funcs = &amp;dumb_gem_obj_funcs; /* 设置该gem object的操作函数 */ drm_gem_object_init(dev, gem_obj, size); // 3. 利用GEM方法生成一个gem_obj对应的handle ret = drm_gem_handle_create(file_priv, gem_obj, &amp;args-&gt;handle); if(ret){ log(); } dumb_buffer_obj-&gt;start_page = alloc_pages(GFP_KERNEL, get_order(size)); if (!dumb_buffer_obj-&gt;start_page) { err(\"alloc_pages() failed\"); return -ENOMEM; } dumb_buffer_obj-&gt;size = size; // 4. 赋值返回参数 args-&gt;size = size; args-&gt;pitch = args-&gt;width*args-&gt;bpp/8; //TODO: 应计算得到 log(\"create dumb_buffer_obj:%p success, return handle:%d\", dumb_buffer_obj, args-&gt;handle); return 0; } static int dumb_map_offset_gem_impl(struct drm_file* file_priv, struct drm_device* dev, uint32_t handle, uint64_t* offset) { int ret; log(\"handle:%d\", handle); // 利用GEM方法得到一个offset ret =drm_gem_dumb_map_offset(file_priv, dev, handle, offset); if(ret){ err(\"drm_gem_dumb_map_offset failed. ret:%d\", ret); }else{ log(\"get offset:0x%llx from drm_gem_dumb_map_offset()\", *offset); } return ret; } static int dumb_release_gem_impl(struct drm_file *file_priv, struct drm_device *dev, u32 handle){ struct drm_gem_object *gem_obj; struct dumb_buffer_object *dumb_buffer_obj = NULL; log(\"\"); gem_obj = drm_gem_object_lookup(file_priv, handle); dumb_buffer_obj = DRM_GEM_OBJECT_TO_DUMB_BUFFER_OBJ(gem_obj); log(\"free page:%p\",dumb_buffer_obj-&gt;start_page); free_pages((unsigned long)page_address(dumb_buffer_obj-&gt;start_page), get_order(dumb_buffer_obj-&gt;size)); kfree(dumb_buffer_obj); return 0; } static struct drm_driver dumb_driver = { .driver_features = DRIVER_GEM, .fops = &amp;dumb_driver_fops, .release = dumb_release, .dumb_create = dumb_create_gem_impl, .dumb_map_offset = dumb_map_offset_gem_impl, .dumb_destroy = dumb_release_gem_impl, .name = DRIVER_NAME, .desc = DRIVER_DESC, .date = DRIVER_DATE, .major = DRIVER_MAJOR, .minor = DRIVER_MINOR, }; static int __init dumb_drv_init(void) { int ret; struct platform_device* pdev = NULL; log(\"build time: %s %s\", __DATE__, __TIME__); pdev = platform_device_register_simple(DRIVER_NAME, -1, NULL, 0); if (IS_ERR(pdev)) return PTR_ERR(pdev); if (!devres_open_group(&amp;pdev-&gt;dev, NULL, GFP_KERNEL)) { ret = -ENOMEM; goto out_unregister; } dumb_device = devm_drm_dev_alloc(&amp;pdev-&gt;dev, &amp;dumb_driver, struct dumb_device, drm); if (IS_ERR(dumb_device)) { ret = PTR_ERR(dumb_device); goto out_devres; } ret = drm_dev_register(&amp;dumb_device-&gt;drm, 0); if (ret) goto out_devres; dumb_device-&gt;platform_dev = pdev; log(\"Finish\"); return 0; out_devres: devres_release_group(&amp;pdev-&gt;dev, NULL); out_unregister: platform_device_unregister(pdev); return ret; } static void __exit dumb_drv_exit(void) { struct platform_device *pdev = dumb_device-&gt;platform_dev; log(); drm_dev_unregister(&amp;dumb_device-&gt;drm); devres_release_group(&amp;pdev-&gt;dev, NULL); platform_device_unregister(pdev); } module_init(dumb_drv_init); module_exit(dumb_drv_exit); MODULE_AUTHOR(\"kevin\"); MODULE_DESCRIPTION(DRIVER_DESC); MODULE_LICENSE(\"GPL\"); 2.1.2.2 流程图 2.2 PRIME PRIME 在 DRM 驱动中其实是一种buffer共享机制，它是基于 dma-buf 来实现的. 2010年2月9日，NVIDIA 官方发布了一项新的双显卡技术 —— Optimus Technology。该技术主要运用于带双显卡的笔记本（集成显卡+独立显卡），可以根据当前集成显卡的工作负载，自动的将一部分图形任务交给独立显卡去处理，以此来达到功耗和性能的最佳平衡。举例来说，一个带 Intel 集成显卡和 NVIDIA 独立显卡的笔记本，通常将集成显卡做为默认显卡，且充当了 Display Controller 的角色。当用户使用办公软件时，由于需要渲染的任务量不多，此时直接由 Intel 集成显卡来完成。而当用户玩3D游戏时，由于图形渲染的负载较重，此时系统会将部分或全部的任务交给 NVIDIA 独立显卡去处理，等处理完后再将结果送回给集成显卡做最后的合成显示。而这整个过程都是由软硬件自动完成的，中间无需人为干预，用户体验十分流畅。只可惜，该技术只能用在 Windows 系统上，Linux 系统不支持。 当时的 Linux 开源社区，Dave Airlie （RedHat Graphics 工程师，DRM 社区 maintainer）在业余时间里研究起 Optimus 技术，并琢磨着怎样在 Linux 平台上实现类似的功能。结果不到2周时间，他就做出了该方案的原型设计，并在自己的笔记本上（Intel集成显卡+ATI独立显卡）实现了该功能的验证。 他将这项技术命名为“PRIME”。 命名为PRIME 主要是映射Optimus Optimus Prime 就是变形金刚 擎天柱的名字！ NVIDIA 当初给他们 Optimus 技术命名的精妙之处：擎天柱本身所具有的变形能力，形象的表达了 Optimus 这项技术可以在功耗和性能之间来回自由变换。 Dave 将他这项 Linux 下的技术命名为 “PRIME”，其实是很巧妙的玩了一把文字游戏，隐晦的告诉大家：DRM Prime 技术就是用来对标 NVIDIA Optimus 技术的。 2.2.1 PRIME的基本实现 为了实现设备间的buffer共享，需要有一套机制来导出、导入buffer。提供buffer的驱动负责导出buffer，使用buffer的设备导入buffer。 DMA_BUF已经提供了这样一套机制。 在DRM的实现中就使用了这套机制，具体是通过借助dmabuf fd来完成的。 导出buffer的驱动对外提供一个fd，来代表一个buffer， 而要使用这个buffer的其他设备驱动就通过这个fd导入buffer，从而访问该buffer。 在DRM中我们需要实现一定的DMA_BUF接口来支持buffer的导出、导入。 下图是一个在不同设备驱动之间导入、导出dma_buf的简单示意图 2.2.1.1 export驱动实现 在驱动的实现中主要添加 .prime_handle_to_fd 和 .prime_fd_to_handle_impl的实现。 interface 实现功能 prime_handle_to_fd 需要实现buffer handle 到dmabuf fd的转换 prime_fd_to_handle_impl 需要实现dmabuf fd 到buffer handle的转换 下面是支持buffer导出的简单实现. 在前面code的基础上添加了对dma-buf export的支持,主要是对struct dma_buf_ops的实现。 结构dma_buf_ops struct dma_buf_ops结构如下： #include &lt;linux/module.h&gt; #include &lt;linux/platform_device.h&gt; #include &lt;linux/slab.h&gt; #include &lt;linux/dma-buf.h&gt; #include &lt;drm/drm_drv.h&gt; #include &lt;drm/drm_file.h&gt; #include &lt;drm/drm_ioctl.h&gt; #define DRIVER_NAME \"drm prime driver\" #define DRIVER_DESC \"test drm prime driver\" #define DRIVER_DATE \"20191124\" #define DRIVER_MAJOR 1 #define DRIVER_MINOR 0 #define log(fmt, args...) printk(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define err(fmt, args...) printk(\"\\033[35m%s():%d \" fmt \"\\033[0m\\n\", __func__, __LINE__, ##args) struct dumb_device { struct drm_device drm; struct platform_device* platform_dev; }; #define MAX_NUM 10 static struct dumb_device* dumb_device = NULL; static struct page* pages[MAX_NUM] = {0}; static u32 buffer_size[MAX_NUM] = {0}; static int page_idx = 0; static int prime_dmabuf_attach_impl(struct dma_buf* dmabuf, struct dma_buf_attachment* attachment) { log(\"attach dmabuf to device, return attachment\"); return 0; } static void prime_dmabuf_detach_impl(struct dma_buf* dmabuf, struct dma_buf_attachment* attachment) { log(\"detach dmabuf\"); } static struct sg_table* prime_dmabuf_map_impl(struct dma_buf_attachment* attachment, enum dma_data_direction dir) { struct page* page = attachment-&gt;dmabuf-&gt;priv; struct sg_table* table; int err; log(\"page:%p\", page); table = kmalloc(sizeof(*table), GFP_KERNEL); if (!table) { log(); return ERR_PTR(-ENOMEM); } // 生成散列表并赋值 err = sg_alloc_table(table, 1, GFP_KERNEL); if (err) { log(); kfree(table); return ERR_PTR(err); } sg_set_page(table-&gt;sgl, page, PAGE_SIZE, 0); sg_dma_address(table-&gt;sgl) = dma_map_page(&amp;(dumb_device-&gt;platform_dev-&gt;dev), page, 0, PAGE_SIZE, dir); log(\"map attachment into sg_table and return sg_table\"); return table; } static void prime_dmabuf_unmap_impl(struct dma_buf_attachment* attachment, struct sg_table* table, enum dma_data_direction dir) { log(); dma_unmap_page(attachment-&gt;dev, sg_dma_address(table-&gt;sgl), PAGE_SIZE, dir); sg_free_table(table); kfree(table); } static void prime_dmabuf_release_impl(struct dma_buf* dma_buf) { struct page* page = dma_buf-&gt;priv; log(\"put_page:%p\", page); put_page(page); } static int prime_dmabuf_vmap_impl(struct dma_buf* dma_buf, struct dma_buf_map* map) { void* vaddr = NULL; struct page* page = dma_buf-&gt;priv; vaddr = vmap(&amp;page, 1, 0, PAGE_KERNEL); dma_buf_map_set_vaddr(map, vaddr); log(\"mapping page:%p, get virtual addr:%p\", page, vaddr); return 0; } static void prime_dmabuf_vunmap_impl(struct dma_buf* dma_buf, struct dma_buf_map* map) { log(\"unmapping addr:%p\", map-&gt;vaddr); vunmap(map-&gt;vaddr); } static int prime_dmabuf_mmap_impl(struct dma_buf* dma_buf, struct vm_area_struct* vma) { struct page* page = dma_buf-&gt;priv; log(); return remap_pfn_range(vma, vma-&gt;vm_start, page_to_pfn(page), PAGE_SIZE, vma-&gt;vm_page_prot); } static const struct dma_buf_ops exp_dmabuf_ops = { .attach = prime_dmabuf_attach_impl, .detach = prime_dmabuf_detach_impl, .map_dma_buf = prime_dmabuf_map_impl, .unmap_dma_buf = prime_dmabuf_unmap_impl, .release = prime_dmabuf_release_impl, .mmap = prime_dmabuf_mmap_impl, .vmap = prime_dmabuf_vmap_impl, .vunmap = prime_dmabuf_vunmap_impl, }; static int prime_handle_to_fd_impl(struct drm_device* dev, struct drm_file* file_priv, uint32_t handle, uint32_t flags, int* prime_fd) { DEFINE_DMA_BUF_EXPORT_INFO(exp_info); struct dma_buf* dmabuf; log(\"dma_buf -&gt; page:%p\", pages[handle]); exp_info.ops = &amp;exp_dmabuf_ops; exp_info.size = buffer_size[handle]; exp_info.flags= O_CLOEXEC; exp_info.priv = pages[handle]; // buffer和dmabuf建立关联 // 构建一个dma_buf dmabuf = dma_buf_export(&amp;exp_info); if (IS_ERR(dmabuf)) { log(); return -1; } // 为该dma_buf生成fd *prime_fd = dma_buf_fd(dmabuf, O_CLOEXEC); if (prime_fd &lt;= 0) { log(); return -1; } return 0; } static int prime_fd_to_handle_impl(struct drm_device* dev, struct drm_file* file_priv, int prime_fd, uint32_t* handle) { struct dma_buf* dma_buf; struct page* page = NULL; int idx = 0; log(); dma_buf = dma_buf_get(prime_fd); page = dma_buf-&gt;priv; if (dma_buf-&gt;ops == &amp;exp_dmabuf_ops) { for (idx = 0; idx &lt; MAX_NUM; idx++) { if (pages[idx] == page) { break; } } *handle = idx; log(\"return handle:%d\", *handle); } else { log(\"doesn't support\"); return -1; } dma_buf_put(dma_buf); return 0; } static const struct file_operations dumb_driver_fops = { .owner = THIS_MODULE, .open = drm_open, .release = drm_release, .unlocked_ioctl = drm_ioctl, .compat_ioctl = drm_compat_ioctl, }; static void dumb_release(struct drm_device* dev) { log(); } static int dumb_create_impl(struct drm_file *file_priv, struct drm_device *dev, struct drm_mode_create_dumb* args) { u32 size = 0; log(); if(page_idx&gt;=MAX_NUM){ err(\"only support alloc %d buffers\", MAX_NUM); return -ENOMEM; } size = roundup((args-&gt;width * args-&gt;height * args-&gt;bpp/8), PAGE_SIZE); // 分配页面 pages[page_idx] = alloc_pages(GFP_KERNEL, get_order(size)); if (!pages[page_idx]) { err(\"alloc_pages() failed\"); return -ENOMEM; } buffer_size[page_idx] = size; // 赋值返回参数 args-&gt;size = size; args-&gt;pitch = args-&gt;width * args-&gt;bpp/8; args-&gt;handle = page_idx++; return 0; } static int dumb_destroy_impl(struct drm_file *file_priv, struct drm_device *dev, uint32_t handle){ log(\"handle:%d\", handle); //__free_pages(pages[handle], get_order(dumb_buffer_size[handle])); free_pages((unsigned long)page_address(pages[handle]), get_order(buffer_size[handle])); return 0; } static struct drm_driver dumb_driver = { .release = dumb_release, .fops = &amp;dumb_driver_fops, .dumb_create = dumb_create_impl, .dumb_destroy = dumb_destroy_impl, .prime_handle_to_fd = prime_handle_to_fd_impl, .prime_fd_to_handle = prime_fd_to_handle_impl, //其他驱动分配的dma_buf导入到DRM系统 .name = DRIVER_NAME, .desc = DRIVER_DESC, .date = DRIVER_DATE, .major = DRIVER_MAJOR, .minor = DRIVER_MINOR, }; static int __init dumb_drv_init(void) { int ret; struct platform_device* pdev = NULL; log(\"build time: %s %s\", __DATE__, __TIME__); pdev = platform_device_register_simple(DRIVER_NAME, -1, NULL, 0); if (IS_ERR(pdev)) return PTR_ERR(pdev); if (!devres_open_group(&amp;pdev-&gt;dev, NULL, GFP_KERNEL)) { ret = -ENOMEM; goto out_unregister; } // 分配 dumb_device = devm_drm_dev_alloc(&amp;pdev-&gt;dev, &amp;dumb_driver, struct dumb_device, drm); if (IS_ERR(dumb_device)) { ret = PTR_ERR(dumb_device); goto out_devres; } // 注册 ret = drm_dev_register(&amp;dumb_device-&gt;drm, 0); if (ret) goto out_devres; dumb_device-&gt;platform_dev = pdev; log(\"Finish\"); return 0; out_devres: devres_release_group(&amp;pdev-&gt;dev, NULL); out_unregister: platform_device_unregister(pdev); return ret; } static void __exit dumb_drv_exit(void) { struct platform_device *pdev = dumb_device-&gt;platform_dev; log(); drm_dev_unregister(&amp;dumb_device-&gt;drm); devres_release_group(&amp;pdev-&gt;dev, NULL); platform_device_unregister(pdev); } module_init(dumb_drv_init); module_exit(dumb_drv_exit); MODULE_AUTHOR(\"kevin\"); MODULE_DESCRIPTION(DRIVER_DESC); MODULE_LICENSE(\"GPL\"); 2.2.1.1 import驱动实现 import dmabuf 的驱动部分实现较为简单，主要是对下列函数的调用,这些函数正好和上面struct dma_buf_ops的实现相对应。 function 功能 dma_buf_get 由fd得到对应的struct dma_buf dma_buf_attach attach dmabuf 到一个设备，得到一个 dma_buf_attachment dma_buf_map_attachment 把一个dmabuf映射到一个设备的地址空间 dma_buf_unmap_attachment 取消dmabuf在设备地址空间的映射 dma_buf_detach 取消与设备的关联 #include &lt;linux/dma-buf.h&gt; #include &lt;linux/module.h&gt; #include &lt;linux/miscdevice.h&gt; #include &lt;linux/slab.h&gt; #define log(fmt, args...) printk(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) static int test_dma_buf(struct dma_buf* dma_buf) { struct dma_buf_attachment* attachment; struct sg_table* table; struct device* dev; unsigned int reg_addr, reg_size; struct scatterlist* sg; struct page* page; int i = 0; if (!dma_buf) { log(); return -EINVAL; } dev = kzalloc(sizeof(*dev), GFP_KERNEL); if (!dev) { log(); return -ENOMEM; } dev_set_name(dev, \"importer\"); attachment = dma_buf_attach(dma_buf, dev); if (IS_ERR(attachment)) { pr_err(\"dma_buf_attach() failed\\n\"); return PTR_ERR(attachment); } log(\"call dma_buf_map_attachment to get sg_table\"); table = dma_buf_map_attachment(attachment, DMA_BIDIRECTIONAL); if (IS_ERR(table)) { pr_err(\"dma_buf_map_attachment() failed\\n\"); dma_buf_detach(dma_buf, attachment); return PTR_ERR(table); } log(\"table-&gt;nents:%d\", table-&gt;nents); sg = table-&gt;sgl; for (i = 0; i &lt; table-&gt;nents; i++) { reg_addr = sg_dma_address(sg); reg_size = sg_dma_len(sg); page = sg_page(sg); log(\"addr = 0x%08x, size = 0x%08x %p %p\\n\", reg_addr, reg_size, page, page_address(page)); sg = sg_next(sg); } // do something on dma-buf log(\"unmap and detach dma_buf\"); dma_buf_unmap_attachment(attachment, table, DMA_BIDIRECTIONAL); dma_buf_detach(dma_buf, attachment); return 0; } static long importer_ioctl(struct file* filp, unsigned int cmd, unsigned long arg) { int fd; struct dma_buf* dma_buf; if (copy_from_user(&amp;fd, (void __user*)arg, sizeof(int))) { log(\"copy_from_user() failed\"); return -EFAULT; } dma_buf = dma_buf_get(fd); log(\"get dma_buf:%p by fd:%d\", dma_buf, fd); if (IS_ERR(dma_buf)) { log(); return PTR_ERR(dma_buf); } test_dma_buf(dma_buf); return 0; } static struct file_operations importer_fops = { .owner = THIS_MODULE, .unlocked_ioctl = importer_ioctl, }; static struct miscdevice mdev = { .minor = MISC_DYNAMIC_MINOR, .name = \"importer\", .fops = &amp;importer_fops, }; static int __init importer_init(void) { log(); return misc_register(&amp;mdev); } static void __exit importer_exit(void) { log(); misc_deregister(&amp;mdev); } module_init(importer_init); module_exit(importer_exit); MODULE_LICENSE(\"GPL v2\"); 2.2.1.1 进程间分享fd 如果要不同设备中共享buffer，就需要导入dmabuf到不同的设备中，这就涉及到一个fd共享的问题。我们知道fd是和进程相关的，每个进程都有它自己的文件描述符表，不同进程的fd表是不同的。所以不能把一个进程的fd直接传递给另一个进程中使用，需要通过一定的方法来共享fd，也就是fd的跨进程传递。 这个方法就是UNIX域的socket接口。 代码如下： 发送方代码： #define SHARE_DMABUF_PATH \"./share_dmabuf_file\" static void send_fd(int fd) { int ret = 0; char c = 0; struct iovec iov[1]; iov[0].iov_base = &amp;c; iov[0].iov_len = 1; int sockfd = 0; struct sockaddr_un addr; bzero(&amp;addr, sizeof(addr)); addr.sun_family = AF_UNIX; strcpy(addr.sun_path, SHARE_DMABUF_PATH); sockfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sockfd &lt; 0) { perror(\"socket error\"); exit(-1); } ret = connect(sockfd, (struct sockaddr *)&amp;addr, sizeof(addr)); if(ret &lt;0){ perror(\"connect() failed\"); exit(0); } int cmsgsize = CMSG_LEN(sizeof(int)); struct cmsghdr* cmptr = (struct cmsghdr*)malloc(cmsgsize); if(cmptr == NULL){ err_exit(); } cmptr-&gt;cmsg_level = SOL_SOCKET; cmptr-&gt;cmsg_type = SCM_RIGHTS; cmptr-&gt;cmsg_len = cmsgsize; struct msghdr msg; msg.msg_iov = iov; msg.msg_iovlen = 1; msg.msg_name = NULL; msg.msg_namelen = 0; msg.msg_control = cmptr; msg.msg_controllen = cmsgsize; *(int *)CMSG_DATA(cmptr) = fd; ret = sendmsg(sockfd, &amp;msg, 0); if (ret == -1){ perror(\"sendmsg() failed.\"); err_exit(); } free(cmptr); close(sockfd); } 接收方代码 static int recv_fd(int sock) { struct cmsghdr* pcmsg = NULL; union { struct cmsghdr cm; char control[CMSG_SPACE(sizeof(int))]; } control_un; char buf; struct iovec iov[1]; iov[0].iov_base = &amp;buf; iov[0].iov_len = sizeof(buf); struct msghdr msg; msg.msg_iov = iov; msg.msg_iovlen = 1; msg.msg_name = NULL; msg.msg_namelen = 0; msg.msg_control = control_un.control; msg.msg_controllen = sizeof(control_un.control);; int ret = recvmsg(sock, &amp;msg, 0); if (ret == -1) { printf(\"\\033[31m%s:%d sock:%d err:%m \\033[0m\\n\\n\",__func__,__LINE__, sock); exit(1); } pcmsg = CMSG_FIRSTHDR(&amp;msg); int fd = *(int *)CMSG_DATA(pcmsg); return fd; } static int recv_prime_fd_from_socket(void){ int sockfd = 0; struct sockaddr_un addr; unlink(SHARE_DMABUF_PATH); addr.sun_family = AF_UNIX; strcpy(addr.sun_path, SHARE_DMABUF_PATH); int prime_fd; int clientfd; struct sockaddr cliaddr; socklen_t clilen; unsigned int len = strlen(addr.sun_path) + sizeof(addr.sun_family); sockfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sockfd &lt; 0) { perror(\"socket error\"); exit(-1); } if (bind(sockfd, (struct sockaddr*)&amp;addr, len) &lt; 0) { perror(\"bind error\"); close(sockfd); exit(-1); } listen(sockfd, 2); clientfd = accept(sockfd, (struct sockaddr *)&amp;cliaddr, &amp;clilen); if(clientfd&lt;=0){ log(\"clientfd:%d %m\", clientfd); exit(-1); } prime_fd = recv_fd(clientfd); log(\"Recv prime_fd: %d\", prime_fd); return prime_fd; } 2.2.1.2 测试程序 基于上述进程间分享fd的方法，结合PRIME export和import驱动实现，可以写出如下测试PRIME的code 导出 prime 部分 #include &lt;fcntl.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;xf86drm.h&gt; #include &lt;sys/socket.h&gt; #include &lt;sys/un.h&gt; #define SHARE_DMABUF_PATH \"./tt\" #define log(fmt, args...) printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define err_return(fmt, args...) do{ \\ printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args); \\ return -1;\\ }while(0) #define err_exit(fmt, args...) do{ \\ printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args); \\ exit(1);\\ }while(0) struct buffer_object { uint32_t width; uint32_t height; uint32_t pitch; uint32_t handle; uint32_t size; int prime_fd; }; struct buffer_object buf; static int create_dumb_get_prime_fd(int fd, struct buffer_object *bo){ int ret = 0; struct drm_mode_create_dumb create = {}; create.width = bo-&gt;width; create.height = bo-&gt;height; create.bpp = 4*8; //ARGB ret = drmIoctl(fd, DRM_IOCTL_MODE_CREATE_DUMB, &amp;create); if(ret){ err_return(\"drmIoctl(DRM_IOCTL_MODE_CREATE_DUMB) failed\"); } ret = drmPrimeHandleToFD(fd, create.handle, DRM_CLOEXEC, &amp;bo-&gt;prime_fd); if(ret){ err_return(\"drmPrimeHandleToFD() failed\"); } bo-&gt;handle = create.handle; log(\"get handle:%d prime fd:%d\", create.handle, bo-&gt;prime_fd); return 0; } static void destroy_fd(int fd, struct buffer_object *bo){ int ret = 0; uint32_t handle = 0; struct drm_mode_destroy_dumb destroy = {}; ret = drmPrimeFDToHandle(fd, bo-&gt;prime_fd, &amp;handle); if(ret){ err_exit(\"drmFDToPrimeHandle() failed\"); } log(\"handle:%d vs bo-&gt;handle:%d\", handle, bo-&gt;handle); destroy.handle = handle; drmIoctl(fd, DRM_IOCTL_MODE_DESTROY_DUMB, &amp;destroy); } int main(int argc, char **argv){ int fd; fd = open(\"/dev/dri/card0\", O_RDWR | O_CLOEXEC); if(fd&lt;=0){ printf(\"%s():%d\\n\", __func__,__LINE__); return -1; } buf.width = 1024; buf.height = 1; create_dumb_get_prime_fd(fd, &amp;buf); send_fd(buf.prime_fd); printf(\"press any key to exit\\n\"); getchar(); destroy_fd(fd, &amp;buf); close(fd); return 0; } 导入prime 部分 #include &lt;fcntl.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/socket.h&gt; #include &lt;sys/un.h&gt; #include &lt;sys/ioctl.h&gt; #define log(fmt, args...) printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args) #define SHARE_DMABUF_PATH \"./tt\" #define err_return(fmt, args...) do{ \\ printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args); \\ return -1;\\ }while(0) #define err_return_void(fmt, args...) do{ \\ printf(\"%s():%d \" fmt \"\\n\", __func__, __LINE__, ##args); \\ return;\\ }while(0) int main(int argc, char **argv){ int ret = 0; int fd; int prime_fd = 5; fd = open(\"/dev/importer\", O_RDWR | O_CLOEXEC); if(fd&lt;=0){ printf(\"%s():%d\\n\", __func__,__LINE__); return -1; } prime_fd = recv_prime_fd_from_socket(); // 把fd传递给import驱动进行访问 ret = ioctl(fd, 0, &amp;prime_fd); if(ret&lt;0){ err_return(\"ioctl failed\"); } close(prime_fd); close(fd); return 0; } 2.2.2 基于GEM的PRIME实现 下图展示了基于GEM如何去实现PRIME特性，代码冗长，就不再贴出。 在实现driver时，主要工作有两个方面。 实现 struct drm_driver，如图所示，就是各函数指针直接设定为GEM的对应函数即可 实现 drm_gem_object_funcs. 只要实现图中drm_gem_object_funcs 各项即可。它所要实现接口对应调用关系如图所示，从中也可看出它们需要实现的内容。 2.3 系统现有的三种实现 2.3.1 share memory 它是基于GEM和share memory实现的一组DRM操作，方便上层使用。 /** * DRM_GEM_SHMEM_DRIVER_OPS - Default shmem GEM operations * * This macro provides a shortcut for setting the shmem GEM operations in * the &amp;drm_driver structure. * 这个宏定义了一组shmem 的GEM操作，利用它可以快捷的在drm_driver结构中定义GEM操作。 * * 在udl, hyperv, vkms, tiny, mgag200和gud 驱动中有用到它，可做参考。 * */ #define DRM_GEM_SHMEM_DRIVER_OPS \\ .prime_handle_to_fd = drm_gem_prime_handle_to_fd, \\ .prime_fd_to_handle = drm_gem_prime_fd_to_handle, \\ .gem_prime_import_sg_table = drm_gem_shmem_prime_import_sg_table, \\ .gem_prime_mmap = drm_gem_prime_mmap, \\ .dumb_create = drm_gem_shmem_dumb_create // GEM 提供的可供driver使用的struct file_operations #define DEFINE_DRM_GEM_FOPS(name) \\ static const struct file_operations name = {\\ .owner = THIS_MODULE,\\ .open = drm_open,\\ .release = drm_release,\\ .unlocked_ioctl = drm_ioctl,\\ .compat_ioctl = drm_compat_ioctl,\\ .poll = drm_poll,\\ .read = drm_read,\\ .llseek = noop_llseek,\\ .mmap = drm_gem_mmap,\\ } 在现有的udl, hyperv, vkms, tiny, mgag200和gud 驱动中，都使用了DEFINE_DRM_GEM_FOPS 和DRM_GEM_SHMEM_DRIVER_OPS 来简化driver的开发。 shem_helper的实现，主要就是针对share memory，实现drm_gem_object_funcs中定义的各个函数接口。 关系图如下： 2.3.1.1 create dumb 实现流程图如下： 2.3.1.2 mmap 实现流程图如下： 2.3.1.3 其他关于GEM的实现 实现流程图如下： 2.3.2 CMA CMA: Contiguous Memory Allocator， CMA是一种内存分配机制，确保分配物理上连续的内存区域，它特别适用于需要具有连续内存的设备（例如显示控制器）的情况。 DRM的CMA helper提供了一种分配既具有物理连续性又适用于DMA（直接内存访问）操作的内存的方法，适合在硬件缺乏IOMMU（输入/输出内存管理单元）来映射分散的缓冲区的场景使用。 /** * DRM_GEM_CMA_DRIVER_OPS_WITH_DUMB_CREATE - CMA GEM driver operations * @dumb_create_func: callback function for .dumb_create * * This macro provides a shortcut for setting the default GEM operations in the * &amp;drm_driver structure. * * This macro is a variant of DRM_GEM_CMA_DRIVER_OPS for drivers that * override the default implementation of &amp;struct rm_driver.dumb_create. Use * DRM_GEM_CMA_DRIVER_OPS if possible. Drivers that require a virtual address * on imported buffers should use * DRM_GEM_CMA_DRIVER_OPS_VMAP_WITH_DUMB_CREATE() instead. * * 使用到它的现有驱动有: aspeed, sti vc4 hisilicon ingenic shmobile xlnx imx sun4i tiny meson 等 */ #define DRM_GEM_CMA_DRIVER_OPS_WITH_DUMB_CREATE(dumb_create_func) \\ .dumb_create = (dumb_create_func), \\ .prime_handle_to_fd = drm_gem_prime_handle_to_fd, \\ .prime_fd_to_handle = drm_gem_prime_fd_to_handle, \\ .gem_prime_import_sg_table = drm_gem_cma_prime_import_sg_table, \\ .gem_prime_mmap = drm_gem_prime_mmap #define DRM_GEM_CMA_DRIVER_OPS \\ DRM_GEM_CMA_DRIVER_OPS_WITH_DUMB_CREATE(drm_gem_cma_dumb_create) cma_helper的实现，主要就是针对CMA的特性调用DMA的接口实现drm_gem_object_funcs中定义的各个函数接口。 一些在CMA中实现的函数 function 功能 drm_gem_cma_create 分配由CMA内存支持的GEM对象。 drm_gem_cma_dumb_create_internal 为DRM帧缓冲创建内存区域。 drm_gem_cma_dumb_map_offset 将DRM帧缓冲的内存区域映射到用户空间。 drm_gem_cma_mmap 为用户空间访问映射分配的内存。 drm_gem_cma_free_object 释放GEM对象。 drm_gem_cma_prime_get_sg_table 获取导入PRIME缓冲区的散射/聚集表。 drm_gem_cma_prime_import_sg_table 从散射/聚集表导入GEM对象。 drm_gem_cma_prime_mmap 为PRIME mmap映射GEM对象。 drm_gem_cma_prime_vmap和drm_gem_cma_prime_vunmap 为PRIME映射和取消映射GEM对象。 关系图如下： 2.3.2.1 create dumb 实现流程图如下： 2.3.2.2 mmap 实现流程图如下： 2.3.2.3 其他关于GEM的实现 实现流程图如下： 2.3.3 VRAM VRAM: Video RAM VRAM是显卡上的专用内存，用于存储图像、纹理、帧缓冲区和其他与图形相关的数据。在显卡中，VRAM是用于显示输出的内存池。它通常位于显卡芯片上，具有高带宽和低延迟，适用于图形渲染。VRAM的大小直接影响显卡的性能。更大的VRAM允许存储更多图像数据，从而提高图形处理速度。 /** * define DRM_GEM_VRAM_DRIVER - default callback functions for \\ &amp;struct drm_driver * * Drivers that use VRAM MM and GEM VRAM can use this macro to initialize * &amp;struct drm_driver with default functions. * * 使用到它的现有驱动有: ast, tiny vboxvideo */ #define DRM_GEM_VRAM_DRIVER \\ .debugfs_init = drm_vram_mm_debugfs_init, \\ .dumb_create = drm_gem_vram_driver_dumb_create, \\ .dumb_map_offset = drm_gem_ttm_dumb_map_offset, \\ .gem_prime_mmap = drm_gem_prime_mmap vram_helper的实现，主要是对TTM接口的封装调用来实现drm_gem_object_funcs中定义的各个函数接口。 GEM、 TTM使用场景 GEM通常用于UMA（Unified Memory Architecture）设备，而TTM更适用于具有专用视频RAM的设备。 关系图如下： 2.3.3.1 create dumb 实现流程图如下： 2.3.3.2 mmap 实现流程图如下： 2.3.3.3 其他关于GEM的实现 实现流程图如下： 3.KMS 3.1 相关概念 KMS（Kernel Mode Setting）是DRM框架的一个重要模块， 主要两个功能： 显示参数设置： KMS负责设置显卡或图形适配器的模式，包括分辨率、刷新率、电源状态（休眠唤醒）等。 显示画面控制： KMS管理显示缓冲区的切换、多图层的合成方式，以及每个图层的显示位置。 KMS的组成部分： DRM FrameBuffer： 这是一个软件抽象，与硬件无关，描述了图层显示内容的信息，如宽度、高度、像素格式和行距等。 Planes： 基本的显示控制单元，每个图像都有一个Plane。Planes的属性控制着图像的显示区域、翻转方式和色彩混合方式。 CRTC： CRTC负责将要显示的图像转化为底层硬件上的具体时序要求。它还负责帧切换、电源控制和色彩调整，可以连接多个Encoder，实现屏幕复制功能。 Encoder： 将内存中的像素转换成显示器所需的信号。 Connector： 连接器负责硬件设备的接入，如HDMI、VGA等，还可以获取设备的EDID和DPMS连接状态。 这些组件共同构成了一个完整的DRM显示控制过程。KMS的目标是适应现代显示设备的逻辑，使其能够更好地支持新特性，如显示覆盖、GPU加速和硬件光标等功能。 3.2 重要结构体 3.3 函数流程 devm_drm_dev_alloc() drm_vblank_init() drm_mode_config_init() drmModeAddFb() init CRTC init plane init connector 3.4 传统实现 drmModeSetCrtc 3.5 atomic实现 smaple code #define _GNU_SOURCE #include &lt;errno.h&gt; #include &lt;fcntl.h&gt; #include &lt;stdbool.h&gt; #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;sys/mman.h&gt; #include &lt;time.h&gt; #include &lt;unistd.h&gt; #include &lt;xf86drm.h&gt; #include &lt;xf86drmMode.h&gt; struct buffer_object { uint32_t width; uint32_t height; uint32_t pitch; uint32_t handle; uint32_t size; uint32_t fb_id; }; struct buffer_object buf; static int modeset_create_fb(int fd, struct buffer_object *bo) { struct drm_mode_create_dumb create = {}; struct drm_mode_map_dumb map = {}; create.width = bo-&gt;width; create.height = bo-&gt;height; create.bpp = 32; drmIoctl(fd, DRM_IOCTL_MODE_CREATE_DUMB, &amp;create); bo-&gt;pitch = create.pitch; bo-&gt;size = create.size; bo-&gt;handle = create.handle; drmModeAddFB(fd, bo-&gt;width, bo-&gt;height, 24, 32, bo-&gt;pitch,bo-&gt;handle, &amp;bo-&gt;fb_id); // drmModeAddFB2 可以传递多个handle // drmModeAddFB(fd, bo-&gt;width, bo-&gt;height, 24, 32, bo-&gt;pitch, // bo-&gt;handle, &amp;bo-&gt;fb_id); //--&gt; calling mode_config_funcs.fb_create() map.handle = create.handle; drmIoctl(fd, DRM_IOCTL_MODE_MAP_DUMB, &amp;map); return 0; } static void modeset_destroy_fb(int fd, struct buffer_object *bo) { struct drm_mode_destroy_dumb destroy = {}; drmModeRmFB(fd, bo-&gt;fb_id); destroy.handle = bo-&gt;handle; drmIoctl(fd, DRM_IOCTL_MODE_DESTROY_DUMB, &amp;destroy); } static uint32_t get_property_id(int fd, drmModeObjectProperties *props, const char *name) { drmModePropertyPtr property; uint32_t i, id = 0; for (i = 0; i &lt; props-&gt;count_props; i++) { property = drmModeGetProperty(fd, props-&gt;props[i]); if (!strcmp(property-&gt;name, name)){ id = property-&gt;prop_id; } drmModeFreeProperty(property); if (id) break; } return id; } int main(int argc, char **argv) { int fd; drmModeConnector *conn; drmModeRes *res; drmModePlaneRes *plane_res; drmModeObjectProperties *props; drmModeAtomicReq *req; uint32_t conn_id; uint32_t crtc_id; uint32_t plane_id; uint32_t blob_id; uint32_t property_crtc_id; uint32_t property_mode_id; uint32_t property_active; uint32_t property_fb_id; uint32_t property_crtc_x; uint32_t property_crtc_y; uint32_t property_crtc_w; uint32_t property_crtc_h; uint32_t property_src_x; uint32_t property_src_y; uint32_t property_src_w; uint32_t property_src_h; fd = open(\"/dev/dri/card0\", O_RDWR | O_CLOEXEC); res = drmModeGetResources(fd); crtc_id = res-&gt;crtcs[0]; conn_id = res-&gt;connectors[0]; drmSetClientCap(fd, DRM_CLIENT_CAP_UNIVERSAL_PLANES, 1); plane_res = drmModeGetPlaneResources(fd); plane_id = plane_res-&gt;planes[0]; conn = drmModeGetConnector(fd, conn_id); buf.width = conn-&gt;modes[0].hdisplay; buf.height = conn-&gt;modes[0].vdisplay; modeset_create_fb(fd, &amp;buf); drmSetClientCap(fd, DRM_CLIENT_CAP_ATOMIC, 1); props = drmModeObjectGetProperties(fd, conn_id, DRM_MODE_OBJECT_CONNECTOR); property_crtc_id = get_property_id(fd, props, \"CRTC_ID\"); drmModeFreeObjectProperties(props); props = drmModeObjectGetProperties(fd, crtc_id, DRM_MODE_OBJECT_CRTC); property_active = get_property_id(fd, props, \"ACTIVE\"); property_mode_id = get_property_id(fd, props, \"MODE_ID\"); drmModeFreeObjectProperties(props); drmModeCreatePropertyBlob(fd, &amp;conn-&gt;modes[0], sizeof(conn-&gt;modes[0]), &amp;blob_id); req = drmModeAtomicAlloc(); drmModeAtomicAddProperty(req, crtc_id, property_active, 1); drmModeAtomicAddProperty(req, crtc_id, property_mode_id, blob_id); drmModeAtomicAddProperty(req, conn_id, property_crtc_id, crtc_id); drmModeAtomicCommit(fd, req, DRM_MODE_ATOMIC_ALLOW_MODESET, NULL); drmModeAtomicFree(req); /* get plane properties */ props = drmModeObjectGetProperties(fd, plane_id, DRM_MODE_OBJECT_PLANE); property_crtc_id = get_property_id(fd, props, \"CRTC_ID\"); property_fb_id = get_property_id(fd, props, \"FB_ID\"); property_crtc_x = get_property_id(fd, props, \"CRTC_X\"); property_crtc_y = get_property_id(fd, props, \"CRTC_Y\"); property_crtc_w = get_property_id(fd, props, \"CRTC_W\"); property_crtc_h = get_property_id(fd, props, \"CRTC_H\"); property_src_x = get_property_id(fd, props, \"SRC_X\"); property_src_y = get_property_id(fd, props, \"SRC_Y\"); property_src_w = get_property_id(fd, props, \"SRC_W\"); property_src_h = get_property_id(fd, props, \"SRC_H\"); drmModeFreeObjectProperties(props); /* atomic plane update */ req = drmModeAtomicAlloc(); drmModeAtomicAddProperty(req, plane_id, property_crtc_id, crtc_id); drmModeAtomicAddProperty(req, plane_id, property_fb_id, buf.fb_id); drmModeAtomicAddProperty(req, plane_id, property_crtc_x, 50); drmModeAtomicAddProperty(req, plane_id, property_crtc_y, 50); drmModeAtomicAddProperty(req, plane_id, property_crtc_w, 320); drmModeAtomicAddProperty(req, plane_id, property_crtc_h, 320); drmModeAtomicAddProperty(req, plane_id, property_src_x, 0); drmModeAtomicAddProperty(req, plane_id, property_src_y, 0); drmModeAtomicAddProperty(req, plane_id, property_src_w, 320 &lt;&lt; 16); drmModeAtomicAddProperty(req, plane_id, property_src_h, 320 &lt;&lt; 16); drmModeAtomicCommit(fd, req, 0, NULL); drmModeAtomicFree(req); printf(\"drmModeAtomicCommit SetPlane done\\n\"); getchar(); modeset_destroy_fb(fd, &amp;buf); drmModeFreeConnector(conn); drmModeFreePlaneResources(plane_res); drmModeFreeResources(res); close(fd); return 0; } drmModeGetProperty drmModeAtomicCommit 4 misc 4.1 modifier 在 DRM（Direct Rendering Manager） 中，modifier 是用于描述图像帧的内存存储方式的标志。让我们详细了解一下： DRM Modifier 是什么？ DRM Modifier 是一个 64 位的、厂商前缀的、半透明的无符号整数。 大多数 modifier 表示图像的具体、厂商特定的平铺格式。 为什么需要 Modifier？ 图像帧在内存中的存储方式可能因硬件和驱动程序而异。 Modifier 描述了像素到内存样本之间的转换机制，以及缓冲区的实际内存存储方式。 常见 Modifier 示例： LINEAR Modifier：最直接的 Modifier，其中每个像素具有连续的存储，像素在内存中的位置可以通过步幅轻松计算。 TILED Modifier：描述了像素以 4x4 块的形式存储，按行主序排列。 其他 Modifier 可能涉及到更复杂的内存访问机制，例如平铺和可能的压缩。 应用场景： DRM Modifier 在跨设备的缓冲区共享中非常重要，例如在不同的图形硬件之间共享图像帧。 它们也用于描述图像帧的格式，例如颜色定义和像素格式。 总之，DRM Modifier 是描述图像帧内存存储方式的关键标志，对于图形硬件和驱动程序之间的交互至关重要。 4.2 gamma 在 DRM（Direct Rendering Manager） 中，gamma 是用于图像颜色校正的一个重要概念。让我详细解释一下： Gamma 是什么？ Gamma 是一个非线性的颜色校正过程，用于调整显示设备的亮度和对比度。 它主要影响图像的中间和暗部，而不是亮部。 Gamma 校正的作用： Gamma 校正用于补偿显示设备的非线性响应。 显示设备（例如显示器、电视）对输入信号的响应不是线性的，而是呈现出一种非线性的亮度-输入关系。 通过应用 Gamma 校正，可以使图像在显示时更接近人眼感知的线性亮度。 Gamma 曲线： Gamma 曲线描述了输入信号和显示亮度之间的关系。 通常，显示设备的 Gamma 曲线是一个幂函数，通常在 2.2 到 2.5 之间。 应用场景： Gamma 校正在图形、视频和游戏中都很重要。 在图像处理和显示中，应用正确的 Gamma 曲线可以确保图像的亮度和对比度在不同设备上保持一致。 总之，Gamma 是一种用于调整显示设备响应的非线性颜色校正方法，以确保图像在不同设备上呈现一致的亮度和对比度。"
  },"/blog/jekyll/2013-03-02-cobalt.html": {
    "title": "Cobalt",
    "keywords": "Jekyll",
    "url": "/blog/jekyll/2013-03-02-cobalt.html",
    "body": "Starboard 在 Cobalt 的上下文中，Starboard 是一个重要的概念。让我为你解释一下： Starboard 是什么？ Starboard 是 Cobalt 的一个关键组件，用于抽象操作系统功能和平台特性，使其能够在不同平台上运行。 它是一个用于移植的抽象层，包含了一系列用于处理不同操作系统和平台特性的实现片段。 Starboard 的目标是将 Cobalt 与底层操作系统和硬件隔离开来，使 Cobalt 能够在不同设备上无缝运行。 如何使用 Starboard？ 在 Cobalt 的源代码中，你会看到 Starboard 的相关目录和文件，这些文件包含了特定平台的实现细节。 Starboard 将平台特定的功能封装在这些文件中，以便 Cobalt 可以在不同平台上运行。 Starboard 的设计目标是只包含 Cobalt 实际使用的平台特定功能，而不包含不必要的部分。 总之，Starboard 是 Cobalt 的一个关键组件，用于处理操作系统和平台差异，使 Cobalt 能够在不同设备上运行。 360 Video"
  }}
